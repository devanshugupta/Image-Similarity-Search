{"cells":[{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1692,"status":"ok","timestamp":1701237567921,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"3Pqb_ontMom6","outputId":"0d15fc93-3cb1-477f-9241-83c1ccd10242"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"DvqhTS7AJT99","executionInfo":{"status":"ok","timestamp":1701237567921,"user_tz":420,"elapsed":14,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import json\n","import pickle\n","import math\n","import random\n","import torch\n","from torchvision.datasets import Caltech101\n","from torchvision import models, transforms\n","from torchvision.models  import resnet50, ResNet50_Weights\n","from scipy.special import softmax\n","\n","from scipy.spatial.distance import euclidean, cosine, minkowski, correlation\n","from sklearn.metrics.pairwise import euclidean_distances\n","from PIL import Image\n"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"A2efsIyOMlzw","executionInfo":{"status":"ok","timestamp":1701237567921,"user_tz":420,"elapsed":8,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Path to drive folder\n","path = '/content/drive/MyDrive/CSE515_Phase3'"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":661,"status":"ok","timestamp":1701237568575,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"wD4xmAlaMq4r","outputId":"ff28f673-24fe-4249-fbd8-416ee905e240"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}],"source":["# Caltech101 Data\n","data = Caltech101(root = f'{path}/data', download = True)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"eP_uwcAAMuOw","executionInfo":{"status":"ok","timestamp":1701237597187,"user_tz":420,"elapsed":28622,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Load the features and label image mapping\n","with open(f'{path}/feature_descriptors.json','r') as fp:\n","    feature_descriptors = json.load(fp)\n","\n","feature_data = pd.DataFrame(feature_descriptors).T.reset_index(names=\"id\")\n","input_vectors = np.array(feature_data[\"layer_3\"].tolist())\n","\n","with open(f'{path}/label_image_map.json','r') as fp:\n","    label_image_map = json.load(fp)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"pn4OcvwCs5tZ","executionInfo":{"status":"ok","timestamp":1701237597188,"user_tz":420,"elapsed":107,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Get the Resnet50 output for an image by hooks in intermediate layers: layer3, avgpool, fc\n","def computeResNet50Vectors(imageId, dataset):\n","\n","    image, label = dataset[imageId]\n","    if (image.mode == 'L') :\n","      image = image.convert(\"RGB\")\n","    with torch.no_grad():\n","        resnet = models.resnet50(pretrained=True)          # Loading the ResNet50 model\n","        hook_output_avg_pool = []                          # list to store the ResNet50 avg pool layer output\n","        hook_output_layer3 = []                            # list to store the ResNet50 layer-3 layer output\n","        hook_output_fc = []                                # list to store the ResNet50 Full Connected layer output\n","\n","\n","        # Hook defined to capture the feature vectors from the avg_pool layer\n","        def hook_fn(module, input, output):\n","            hook_output_avg_pool.append(output)\n","\n","        # Hook defined to capture the feature vectors from the Layer3\n","        def hook_fn_layer3(module, input, output):\n","            hook_output_layer3.append(output)\n","\n","        # Hook defined to capture the feature vectors from the Fully Connected layer\n","        def hook_fn_fc(module, input, output):\n","            hook_output_fc.append(output)\n","\n","\n","        # Registering the defined hook as the forward hook on the avg pool layer on the ResNet 50 model\n","        avgpool_layer = resnet.avgpool\n","        avgpool_layer.register_forward_hook(hook_fn)\n","\n","        # Registering the defined hook as the forward hook on the layer-3 on the ResNet 50 model\n","        layer3 = resnet.layer3\n","        layer3.register_forward_hook(hook_fn_layer3)\n","\n","        # Registering the defined hook as the forward hook on the fully connected layer on the ResNet 50 model\n","        fc_layer = resnet.fc\n","        fc_layer.register_forward_hook(hook_fn_fc)\n","\n","        # Performing tranform operations on the image to resize it and retrieve the tensors from each layer\n","        transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n","        image = transform(image).unsqueeze(0)\n","        resnet(image)                # Passing the image to the ResNet Model\n","\n","\n","        #Storing the output of the hooks in the predefined lists\n","        avgpool_vector = hook_output_avg_pool[0][0]\n","        layer3_vector = hook_output_layer3[0][0]\n","        fc_vector = hook_output_fc[0][0]\n","\n","\n","        avgpool_vector = avgpool_vector.squeeze().detach().numpy().reshape(1024, 2)           # Converting the vectors from tensor to numpy to perform mathematical operations\n","        reduced_avg_pool_dimensionality_vector = np.mean(avgpool_vector, axis = 1)            # Dimensional Reduction is perfromed by averaging up the consecutive numbers and reducing the resultant array to 1D\n","\n","        layer3_vector = layer3_vector.detach().numpy()                                        # Converting the vectors from tensor to numpy to perform mathematical operations\n","        reduced_layer3_dimensionality_vector = np.mean(layer3_vector, axis = (1, 2))          # Dimensional Reduction is perfromed by averaging up the 14, 14 slice and reducing the resultant array to 1D\n","\n","        fc_vector = fc_vector.detach().numpy()                                                            # Converting the vectors from tensor to numpy to perform mathematical operations\n","        resnet = softmax(fc_vector)\n","    return (reduced_avg_pool_dimensionality_vector, reduced_layer3_dimensionality_vector, fc_vector, resnet)  # returning the feature descriptors of all 3 layers in a tuple"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"JPVG602TVg5h","executionInfo":{"status":"ok","timestamp":1701237597189,"user_tz":420,"elapsed":96,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# SVM"]},{"cell_type":"code","source":["class SVM:\n","    def _init_(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n","        self.lr = learning_rate\n","        self.lambda_param = lambda_param\n","        self.n_iters = n_iters\n","        self.w = None\n","        self.b = None\n","\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","\n","        y_ = np.where(y <= 0, -1, 1)\n","\n","        # init weights\n","        self.w = np.zeros(n_features)\n","        self.b = 0\n","\n","        for _ in range(self.n_iters):\n","            for idx, x_i in enumerate(X):\n","                condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n","                if condition:\n","                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n","                else:\n","                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n","                    self.b -= self.lr * y_[idx]\n","\n","\n","    def predict(self, X):\n","        approx = np.dot(X, self.w) - self.b\n","        return np.sign(approx)"],"metadata":{"id":"YGRDoRj9Yi4z","executionInfo":{"status":"ok","timestamp":1701237597189,"user_tz":420,"elapsed":95,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","execution_count":52,"metadata":{"id":"ZvFBkm7NwPTs","executionInfo":{"status":"ok","timestamp":1701237597189,"user_tz":420,"elapsed":94,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# SVM Model for Multi-Label Classification (One-vs-Rest)\n","class MultiLabelSVM:\n","    def __init__(self, learning_rate=0.001, num_epochs=10000):\n","        self.learning_rate = learning_rate\n","        self.num_epochs = num_epochs\n","        self.weights = None\n","        self.bias = None\n","\n","    def train(self, X, y, label):\n","        num_samples, num_features = X.shape\n","        self.weights = np.zeros(num_features)\n","        self.bias = 0\n","\n","        # Convert multi-labels to binary labels (1 for the target label, -1 for others)\n","        binary_labels = np.where(y == label, 1, -1)\n","\n","        for epoch in range(self.num_epochs):\n","            # SVM decision function\n","            scores = np.dot(X, self.weights) + self.bias\n","\n","            # Hinge loss\n","            margin = 1 - binary_labels * scores\n","            margin[margin < 0] = 0\n","\n","            # Gradient descent update\n","            gradient_weights = -np.dot(X.T, binary_labels * (margin > 0))\n","            gradient_bias = -np.sum(binary_labels * (margin > 0))\n","\n","            self.weights -= self.learning_rate * gradient_weights\n","            self.bias -= self.learning_rate * gradient_bias\n","\n","    def predict(self, X):\n","        return np.dot(X, self.weights) + self.bias\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"VKlG0-MrJUHk","executionInfo":{"status":"ok","timestamp":1701237597189,"user_tz":420,"elapsed":92,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Task4b Output"]},{"cell_type":"code","source":["# LSH and get t images code\n","class LSH:\n","  def __init__(self, L, h, input_vectors):\n","    self.L = L\n","    self.h = h\n","    self.W = 0.0\n","    self.vectors = input_vectors\n","    self.lsh_hyperplanes = {}\n","    self.lsh_hyperplane_ranges = {}\n","    self.hash_tables = [{} for _ in range(self.L)]\n","\n","  def create_index(self):\n","    self.init_random_hyperplanes()\n","    self.W = min(self.lsh_hyperplane_ranges.values()) / float(50)\n","\n","    for vector_id, vector in enumerate(self.vectors):\n","      for layer_id in range(self.L):\n","        bucket_key = self.get_bucket_key(vector, layer_id)\n","        if bucket_key not in self.hash_tables[layer_id]:\n","          self.hash_tables[layer_id][bucket_key] = set()\n","        self.hash_tables[layer_id][bucket_key].add(vector_id)\n","\n","  def init_random_hyperplanes(self):\n","    # Define the origin vector with zeros, size should match the number of features (1024)\n","    origin_vector = np.zeros(self.vectors.shape[1])\n","\n","    # Iterate through each layer and hash\n","    for layer_id in range(self.L):\n","      for hash_id in range(self.h):\n","        random_hyperplane_vector = []\n","\n","        # Iterate through each feature\n","        for col in range(self.vectors.shape[1]):\n","          # Get min and max of the current feature across all data points\n","          min_val = self.vectors[:, col].min()\n","          max_val = self.vectors[:, col].max()\n","\n","          # Get a random value between min and max\n","          random_val = random.uniform(min_val, max_val)\n","          random_hyperplane_vector.append(random_val)\n","\n","        random_hyperplane_vector = np.array(random_hyperplane_vector)\n","\n","        # Store the hyperplane vector and its distance from the origin\n","        hyperplane_key = (layer_id, hash_id)\n","        self.lsh_hyperplanes[hyperplane_key] = random_hyperplane_vector\n","        self.lsh_hyperplane_ranges[hyperplane_key] = euclidean(origin_vector, random_hyperplane_vector)\n","\n","  def get_bucket_key(self, vector, layer_id):\n","    bucket_key = []\n","    for hash_id in range(self.h):\n","      hyperplane = self.lsh_hyperplanes[(layer_id, hash_id)]\n","      projected_value = self.project_on_hyperplane(vector, hyperplane)\n","      bucket_key.append(self.assign_vector(projected_value))\n","    return tuple(bucket_key)\n","\n","  def project_on_hyperplane(self, input_vector, lsh_vector):\n","    dp = np.dot(input_vector, lsh_vector)\n","    if dp == 0.0:\n","      return 0\n","    projection = dp/np.dot(lsh_vector, lsh_vector)*lsh_vector\n","    magnitude = np.linalg.norm(projection)\n","    return magnitude\n","\n","  def assign_vector(self, value):\n","    if value < 0:\n","      return math.floor(value/self.W)\n","    else:\n","      return math.ceil(value/self.W)\n","\n","  def query(self, query_vector):\n","    results = []\n","    for layer_id in range(self.L):\n","      bucket_key = self.get_bucket_key(query_vector, layer_id)\n","      results.extend(list(self.hash_tables[layer_id].get(bucket_key, set())))\n","    return results\n","\n","  def get_adjacent_buckets(self, bucket_key, layer_id):\n","    adjacent_keys = []\n","\n","    mini = min(min(self.hash_tables[layer_id].keys()))\n","    maxi = max(max(self.hash_tables[layer_id].keys()))\n","\n","    for delta in range(1, maxi - mini + 1):\n","      for i in range(len(bucket_key)):\n","        modified_key = list(bucket_key)\n","        modified_key[i] -= delta\n","      adjacent_keys.append(tuple(modified_key))\n","      for i in range(len(bucket_key)):\n","        modified_key = list(bucket_key)\n","        modified_key[i] += delta\n","      adjacent_keys.append(tuple(modified_key))\n","\n","    return adjacent_keys\n","\n","  def expanded_lsh_query(self, query_vector, t):\n","    results = self.query(query_vector)\n","\n","    # Check if we need to expand the search\n","    if len(set(results)) >= t:\n","      return results\n","\n","    # Expand to adjacent buckets\n","    for layer_id in range(self.L):\n","      bucket_key = self.get_bucket_key(query_vector, layer_id)\n","      for adj_key in self.get_adjacent_buckets(bucket_key, layer_id):\n","        if adj_key in self.hash_tables[layer_id].keys() and adj_key != bucket_key:\n","          results.extend(list(self.hash_tables[layer_id].get(adj_key, set())))\n","          if len(set(results)) >= t:\n","              return results\n","\n","    return results\n","\n","  def print_bucket_sizes(self):\n","    for layer_id, layer in enumerate(self.hash_tables):\n","      print(f\"Layer {layer_id}:\")\n","      for bucket_key, bucket in layer.items():\n","        print(f\"  Bucket {bucket_key}: {len(bucket)} images\")"],"metadata":{"id":"iGzVpx8Rou3Z","executionInfo":{"status":"ok","timestamp":1701237597189,"user_tz":420,"elapsed":91,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","execution_count":55,"metadata":{"id":"KPbgUET3JUCo","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":91,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["def get_lsh(lsh, query_image_index, t):\n","  resnet_outputs = computeResNet50Vectors(query_image_index, data)\n","  query_vector = resnet_outputs[1]\n","\n","  candidates_union = lsh.expanded_lsh_query(query_vector, t)\n","\n","  candidates_id = list(set(candidates_union))\n","\n","  candidate_vectors = [lsh.vectors[i] for i in candidates_id]\n","\n","  if len(candidates_id) < t:\n","    distances = euclidean_distances([query_vector], input_vectors)[0]\n","    sorted_indices = np.argsort(distances)\n","\n","    similar_images = [(i * 2, distances[i]) for i in sorted_indices[:t]]\n","\n","    return similar_images\n","\n","  # Calculate Euclidean distances\n","  distances = euclidean_distances([query_vector], candidate_vectors)[0]\n","\n","  # Sort candidates by distance\n","  sorted_indices = np.argsort(distances)\n","\n","  # Visualize the t most similar images\n","  similar_images = [(list(candidates_id)[i]*2, distances[i]) for i in sorted_indices[:t]]\n","\n","  return similar_images"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"g7XLTUF-sTYp","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":91,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Load the precalculated LSH\n","def load_lsh_index_from_file(filename):\n","  with open(filename, 'rb') as file:\n","    lsh = pickle.load(file)\n","  return lsh"]},{"cell_type":"code","source":["# display images\n","def display_images(image_indices, dataset, w, h):\n","    fig, axs = plt.subplots(1, len(image_indices), figsize=(30, 10))\n","    for i,(index, score) in enumerate(image_indices):\n","        image_array = np.array(dataset[int(index)][0])  # Convert JpegImageFile to NumPy array\n","        image = Image.fromarray(image_array)\n","        axs[i].imshow(image.resize((w, h)))\n","        axs[i].axis('off')\n","        axs[i].set_title(f\"Image {int(index)}\")\n","    plt.show()"],"metadata":{"id":"aVL7jyHaS3Rb","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":90,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# Get binary features for image\n","def get_binary(feature_spc, images):\n","  #calculating threshold to convert features to boolean values\n","  thresholds = np.mean(feature_spc, axis=0) + np.std(feature_spc, axis=0)\n","  binary_data = (feature_spc > thresholds).astype(int)\n","  binary_imgid_ftr = {}\n","  for i, tupl in enumerate(images):\n","    binary_imgid_ftr[tupl[0]] = binary_data[i]\n","  return binary_imgid_ftr"],"metadata":{"id":"wR57y0hFWZNg","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":89,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# Display Images\n","def display_given_count_images(sorted_keys, number):\n","  fig, axs = plt.subplots(1, len(sorted_keys[:number]), figsize=(30, 10))\n","  for i, index in enumerate(sorted_keys[:number]):\n","      image_array = np.array(data[int(index)][0])\n","      image = Image.fromarray(image_array)\n","      axs[i].imshow(image.resize((200, 200)))\n","      axs[i].axis('off')\n","      axs[i].set_title(f\"Id-{int(index)}\")\n","  plt.show()"],"metadata":{"id":"VOvw3eK6ZYUH","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":88,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Calculate feature significance\n","def feature_wgts_from_relevance(relv_of_pred, binary_ftr):\n","  vry_rel_obj = []\n","  rel_obj = []\n","  irrel_obj = []\n","  vry_irrel_obj = []\n","  for imgId in relv_of_pred.keys():\n","    if relv_of_pred[imgId] == '2':\n","      vry_rel_obj.append(binary_ftr[imgId])\n","    elif relv_of_pred[imgId] == '1':\n","      rel_obj.append(binary_ftr[imgId])\n","    elif relv_of_pred[imgId] == '-1':\n","      irrel_obj.append(binary_ftr[imgId])\n","    elif relv_of_pred[imgId] == '-2':\n","      vry_irrel_obj.append(binary_ftr[imgId])\n","\n","  vry_rel_obj = np.array(vry_rel_obj)\n","  rel_obj = np.array(rel_obj)\n","  irrel_obj = np.array(irrel_obj)\n","  vry_irrel_obj = np.array(vry_irrel_obj)\n","\n","  # Calculating P(f=1|R) P(f=0|R), P(f=1|IR) P(f=0|IR)\n","  Pf1_rel = np.zeros(1024)\n","  Pf0_rel = np.zeros(1024)\n","  Pf1_irrel = np.zeros(1024)\n","  Pf0_irrel = np.zeros(1024)\n","  if len(vry_rel_obj)>0:\n","    Pf1_rel = np.sum(vry_rel_obj, axis = 0) / (len(vry_rel_obj) + 1)\n","  if len(rel_obj)>0:\n","    Pf1_rel += (np.sum(rel_obj, axis = 0) * 0.75) / (len(rel_obj) + 1)\n","  if len(vry_rel_obj)>0 and len(rel_obj)>0:\n","    Pf1_rel = Pf1_rel/2\n","  Pf1_rel[Pf1_rel == 0] = 0.5 / (len(vry_rel_obj) + len(rel_obj) + 1)\n","  Pf0_rel = 1 - Pf1_rel\n","\n","  if len(vry_irrel_obj)>0:\n","    Pf1_irrel = np.sum(vry_irrel_obj, axis = 0) / (len(vry_irrel_obj) + 1)\n","  if len(irrel_obj)>0:\n","    Pf1_irrel += (np.sum(irrel_obj, axis = 0)*0.75) / (len(irrel_obj) + 1)\n","  if len(vry_irrel_obj)>0 and len(irrel_obj)>0:\n","    Pf1_irrel = Pf1_irrel/2\n","  Pf1_irrel[Pf1_irrel == 0] = 0.5 / (len(vry_irrel_obj) + len(irrel_obj) + 1)\n","  Pf0_irrel = 1 - Pf1_irrel\n","\n","  # print(Pf0_rel.min())\n","  # print(Pf1_irrel.min())\n","  P = np.log((Pf1_rel*Pf0_irrel)/(Pf0_rel*Pf1_irrel)) # weights of all the features calculated with respect to the relevance feedback\n","  return P"],"metadata":{"id":"S7FUoszPZR2B","executionInfo":{"status":"ok","timestamp":1701237597190,"user_tz":420,"elapsed":12,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["### RF loop"],"metadata":{"id":"lK9e836OmkN9"}},{"cell_type":"code","source":["# Probability based Relevance Feedback\n","def prob_based_rf():\n","  while True:\n","    query_image = int(input('Enter the query image id: '))\n","    t = int(input('Enter no. of images t: '))\n","    display(data[int(query_image)][0].resize((150,150)))\n","    lsh_index_structure = load_lsh_index_from_file(f'{path}/LSH_index_structures/lsh_index_structure_10_3.pkl')\n","    images = get_lsh(lsh_index_structure, query_image, 30) # Get some initial images and scores\n","\n","    features_of_pred = []\n","    for img_id, score in images:\n","      features_of_pred.append(feature_descriptors[str(img_id)]['layer_3'])\n","\n","    binary_ftr = get_binary(np.array(features_of_pred), images)\n","    relv_of_pred = {}\n","    dis = t # number of images to be dispalyed, can be changed if user wants to give feedback to more or less number of images than intially mentioned\n","\n","    print('Feedback relevancy: 2 for Very relevant | 1 for relevant | -1 for irrelevant | -2 for very irrelevant | 0 to skip->')\n","    display_images(images[:t],data, 200, 200)\n","    User_q=0\n","    for img_id, score in images:\n","      if User_q < t:\n","        relevancy = input(f'Enter the relevancy of {img_id}: ')\n","        relv_of_pred[img_id] = relevancy\n","      User_q+=1\n","\n","    while True:\n","      ftr_weights = feature_wgts_from_relevance(relv_of_pred, binary_ftr)\n","      obj_rel = {}\n","      # weighing the feature vectors of our images\n","      for imgId, score in images:\n","        obj_rel[imgId] = (np.dot(ftr_weights, binary_ftr[imgId]))\n","\n","      sorted_keys = sorted(obj_rel, key=obj_rel.get, reverse=True)\n","\n","      display_given_count_images(sorted_keys, dis)\n","\n","      x = input('Do you want to continue giving feedback[Y/N]')\n","      if x in ['Y','y']:\n","        feedback_cnt = int(input('Enter the Number of images you want to view to give feedback:'))\n","        if(feedback_cnt > dis):\n","          display_given_count_images(sorted_keys, feedback_cnt)\n","        print('Feedback relevancy: 2 for Very relevant | 1 for relevant | -1 for irrelevant | -2 for very irrelevant | 0 if no feedback for an image ->')\n","        for img_id in sorted_keys[:feedback_cnt]:\n","          relevancy = input(f'Enter the relevancy of {img_id}: ')\n","          if relevancy != 0:\n","            relv_of_pred[img_id] = relevancy\n","      else:\n","        break\n","    q = input('Do you have another query?[Y/N]')\n","    if q in ['N','n']:\n","      break"],"metadata":{"id":"7RonhC3GkFXj","executionInfo":{"status":"ok","timestamp":1701237597191,"user_tz":420,"elapsed":12,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","execution_count":64,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Qm8gVXwfXKh9vFODP0ADVg3yc8ZLDANf"},"executionInfo":{"elapsed":79398,"status":"ok","timestamp":1701237814048,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"MsSdkMRMJUFU","outputId":"42c40570-3b8b-46da-8f18-ad951761a97d"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Main FeedBack loop to get t images, User selected approach\n","feedback_model = int(input('Enter the feedback model to use 1)SVM, 2)Probabilistic Relevance feedback model '))\n","if feedback_model==1:\n","  exit = True\n","  query_image = int(input('Enter the query image id: '))\n","  t = int(input('Enter no. of images t: '))\n","  display(data[int(query_image)][0].resize((150,150)))\n","  lsh_index_structure = load_lsh_index_from_file(f'{path}/LSH_index_structures/lsh_index_structure_10_3.pkl')\n","  images = get_lsh(lsh_index_structure, query_image, t) # Get initial t images\n","  if len(images)<t:\n","      print('Not enough Images in LSH Bucket')\n","  tagged={}\n","  labels = ['2','1','-1','-2']\n","  # Create a multi-label SVM classifier using One-vs-Rest\n","  num_classes = 4\n","  svm_classifiers = [MultiLabelSVM() for _ in range(num_classes)]\n","  j = 2\n","  print('Feedback relevancy: 2 for Very relevant | 1 for relevant | -1 for irrelevant | -2 for very irrelevant ->')\n","  while exit:\n","      display_images(images,data, 200, 200)\n","\n","      for img_id, score in images:\n","          relevancy = input(f'Enter the relevancy of {img_id}: ')\n","          tagged[img_id] = [score, relevancy]\n","\n","      X_train,y_train = [],[]\n","      for i in tagged:\n","          X_train.append(feature_descriptors[str(i)]['layer_3'])\n","          y_train.append(tagged[i][1])\n","\n","      for i,label in enumerate(labels):\n","          svm_classifiers[i].train(np.array(X_train), np.array(y_train), label)\n","\n","      # Expand query range\n","      images_new = get_lsh(lsh_index_structure, query_image, j*t)\n","      images_new2 = []\n","      X_test = []\n","      for img_id, score in images_new:\n","          if img_id not in tagged:\n","              images_new2.append((img_id, score))\n","              X_test.append(feature_descriptors[str(img_id)]['layer_3'])\n","      if len(X_test)==0:\n","          print('Not enough Images in LSH Bucket')\n","          break\n","      # Make predictions on the test set\n","      predictions = np.array([classifier.predict(np.array(X_test)) for classifier in svm_classifiers])\n","      # Choose the label with the highest confidence as the predicted label\n","      predicted_labels = np.argmax(predictions, axis=0)\n","\n","      pred = []\n","      for i in predicted_labels:\n","          pred.append(labels[i])\n","\n","      img_train = []\n","      for i in tagged:\n","          img_train.append(((i,tagged[i][0]),tagged[i][1]))\n","\n","      img_test = list(zip(images_new2, pred))\n","      img_train.extend(img_test)\n","      img_train = sorted(img_train, key = lambda x:x[1], reverse = True)\n","\n","      images = []\n","      for i in range(t):\n","          images.append(img_train[i][0])\n","      j+=1\n","      exit = input('Do you want to exit, y/n : ') in ['N','n']\n","elif feedback_model==2:\n","  prob_based_rf()"]},{"cell_type":"code","source":[],"metadata":{"id":"vY2NVVihdK_I","executionInfo":{"status":"ok","timestamp":1701237651146,"user_tz":420,"elapsed":32,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"execution_count":62,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}