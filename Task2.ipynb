{"cells":[{"cell_type":"markdown","metadata":{"id":"REBcfwZ5cmFM"},"source":["Task 2: Implement a program which\n","1.   for each **unique labe**l l, computes the correspending **c most significant clusters** associated with the even numbered Caltec101 images (using DBScan algorithm); the resulting clusters should be visualized both\n","\n","*   as differently colored point clouds in a 2-dimensional **MDS** space, and\n","*   as **groups of image thumbnails**. and\n","\n","2.   for the odd numbered images, predicts the most likely labels using the c label-specific clusters.\n","\n","The system should also output per-label precision, recall, and F1-score values as well as output an overall accuracy\n","value.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19476,"status":"ok","timestamp":1701236989476,"user":{"displayName":"Vishal Reddy Eda","userId":"02842083633786024718"},"user_tz":420},"id":"Djw2OlVMwA13","outputId":"c7551742-36db-4aa7-bc9c-9a2f5e808777"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DR9x_eSVchod"},"outputs":[],"source":["# Imports\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","import json\n","import torch\n","from torchvision.datasets import Caltech101\n","from torchvision.models  import resnet50, ResNet50_Weights\n","from torchvision import transforms\n","\n","import matplotlib.pyplot as plt\n","from scipy.spatial.distance import euclidean, cosine, minkowski, correlation\n","from PIL import Image\n","from sklearn.preprocessing import  minmax_scale\n","from sklearn.cluster import DBSCAN\n","\n","from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","import collections\n","from collections import defaultdict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H25Datxst38s"},"outputs":[],"source":["# path to drive folder\n","path = '/content/drive/MyDrive/CSE515_Phase3'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17856,"status":"ok","timestamp":1701234314775,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"9uEwS__Mv7e5","outputId":"fbe22111-195f-44e2-b1ae-6ccdcd5e808c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}],"source":["# Load the caltech101 dataset\n","data = Caltech101(root = f'{path}/data', download = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08-W3ITOc4to"},"outputs":[],"source":["# Label image mapping\n","with open(f'{path}/label_image_map.json','r') as fp:\n","    label_image_map = json.load(fp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ERbxkZikoY49"},"outputs":[],"source":["# Latent space SVD layer 3\n","with open(f'{path}/latent_spaces/SVD_66_layer_3_latent_weights.json','r') as fp:\n","    latent_features = json.load(fp)"]},{"cell_type":"code","source":["# Methods to calculate per label metrics of Precision, Recall, F1 score and Accuracy\n","def calculate_per_label_metrics(actual_labels, predicted_labels):\n","    unique_labels = set(actual_labels + predicted_labels)\n","    label_to_index = {label: i for i, label in enumerate(unique_labels)}\n","    label_count = len(unique_labels)\n","\n","    true_positives = [0] * label_count\n","    false_positives = [0] * label_count\n","    false_negatives = [0] * label_count\n","\n","    for actual, predicted in zip(actual_labels, predicted_labels):\n","        actual_index = label_to_index[actual]\n","        predicted_index = label_to_index[predicted]\n","\n","        if actual == predicted:\n","            true_positives[actual_index] += 1\n","        else:\n","            false_positives[predicted_index] += 1\n","            false_negatives[actual_index] += 1\n","\n","    precision = [true_positives[i] / (true_positives[i] + false_positives[i] + 1e-10) for i in range(label_count)]\n","    recall = [true_positives[i] / (true_positives[i] + false_negatives[i] + 1e-10) for i in range(label_count)]\n","    f1_score = [2 * (p * r) / (p + r + 1e-10) for p, r in zip(precision, recall)]\n","\n","    return precision, recall, f1_score\n","\n","def calculate_accuracy(actual_labels, predicted_labels):\n","    correct_predictions = sum(1 for a, p in zip(actual_labels, predicted_labels) if a == p)\n","    accuracy = correct_predictions / len(actual_labels)\n","    return accuracy"],"metadata":{"id":"YlFd8_tWq83R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhbNvjsdfnJT"},"outputs":[],"source":["# Multi Dimensional Scaling and stress calculation\n","def stress_function(original_distances, reduced_distances):\n","    stress = np.sqrt(((reduced_distances.ravel() - original_distances.ravel()) ** 2).sum() / ((original_distances.ravel() ** 2).sum()))\n","    return stress\n","\n","def MDS(data, num_dimensions):\n","    num_images, num_features = len(data), len(data[0])\n","    # Random Positions\n","    X = np.random.rand(num_images, num_dimensions)\n","    # Set optimization parameters\n","    max_iterations = 300\n","\n","    original_distances = euclidean_distances(data)\n","    original_distances[original_distances == 0] = 1e-8\n","\n","    # Optimization loop\n","    for iteration in range(max_iterations):\n","        # Compute the current stress value\n","        reduced_distances = euclidean_distances(X)\n","        current_stress = stress_function(original_distances, reduced_distances)\n","        if current_stress < 0.05:\n","            break\n","        # Guttman Transformation\n","        reduced_distances[reduced_distances == 0] = 1e-8\n","        ratio = original_distances / reduced_distances\n","        B = -ratio\n","        B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)\n","        X = 1.0 / original_distances.shape[0] * np.dot(B, X)\n","\n","        reduced_distances = np.sqrt((X**2).sum(axis=1)).sum()\n","\n","    return X, original_distances\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xT4ouGbMc42O"},"outputs":[],"source":["# DBSCAN Methods\n","\n","def get_core_point(distance, epsilon, M):\n","    density_dict = {}\n","    core_point = []\n","    for index, node in enumerate(distance):\n","        epsilon_neib = np.squeeze(np.argwhere(node < epsilon))\n","        #  epsilon\n","        if epsilon_neib.size == 0:\n","            node_density_set = []\n","        #  epsilon\n","        elif epsilon_neib.size == 1:\n","            node_density_set = [int(epsilon_neib)]\n","        #  epsilon\n","        else:\n","            node_density_set = list(epsilon_neib)\n","            if epsilon_neib.size >= M:\n","                core_point.append(index)\n","        density_dict[index] = node_density_set\n","    return density_dict, core_point\n","\n","def get_noise_point(density_dict, core_point, data_length):\n","    noncore_point = set(range(data_length)) - set(core_point)\n","    noise_point = []\n","    for point in noncore_point:\n","        if len(set(density_dict[point]) & set(core_point)) == 0:\n","            noise_point.append(point)\n","    return noise_point\n","\n","def assign_class(density_dict, core_point, data_length, class_list, noise_point):\n","    # core_point.reverse()\n","    cluster_quant = {}\n","    for core in core_point:\n","        if class_list[core] == -1:\n","            class_list[core] = core\n","            cluster_quant[class_list[core]]=1\n","            density_propagation(density_dict, core, class_list, core_point, noise_point, cluster_quant)\n","    sorted_dict = dict(sorted(cluster_quant.items(), key=lambda item: item[1]))\n","    for k in sorted_dict.keys():\n","        visited = []\n","        # print(k)\n","        for epsilon_neib in density_dict[k]:\n","            density_propagation_non_core(density_dict, epsilon_neib, class_list, core_point, visited)\n","    return class_list\n","# Recurrsion method to identify classes and non core points\n","def density_propagation_non_core(density_dict, core, class_list, core_point, visited):\n","    visited.append(core)\n","    for epsilon_neib in density_dict[core]:\n","      if epsilon_neib in core_point and epsilon_neib not in visited:\n","        density_propagation_non_core(density_dict, epsilon_neib, class_list, core_point, visited)\n","      elif class_list[epsilon_neib] == -1:\n","        class_list[epsilon_neib] = class_list[core]\n","\n","# Recurrsion method to identify classes\n","def density_propagation(density_dict, core, class_list, core_point, noise_point, cluster_quant):\n","    # core_point.reverse()\n","    for epsilon_neib in density_dict[core]:\n","        if epsilon_neib and class_list[epsilon_neib] == -1:\n","            if epsilon_neib in core_point:\n","              class_list[epsilon_neib] = class_list[core]\n","              cluster_quant[class_list[core]]+=1\n","              density_propagation(density_dict, epsilon_neib, class_list, core_point, noise_point, cluster_quant)\n","\n","\n","def show_result(class_list, raw_data):\n","    colors = [\n","              '#FF0000', '#FFA500', '#FFFF00', '#00FF00', '#228B22',\n","              '#0000FF', '#FF1493', '#EE82EE', '#000000', '#FFA500',\n","              '#00FF00', '#006400', '#00FFFF', '#0000FF', '#FFFACD',\n","              ]\n","\n","    use_color = {}\n","    total_color = list(dict(collections.Counter(class_list)).keys())\n","    if -1 in total_color:\n","        total_color.remove(-1)\n","    for index, i in enumerate(total_color):\n","        use_color[i] = index\n","    plt.figure(num=1, figsize=(15, 10))\n","    for node, class_ in enumerate(class_list):\n","        if class_ != -1:\n","            plt.scatter(x=raw_data[node,0], y=raw_data[node,1], s=5, marker='o', alpha=0.73, c = colors[use_color[class_]])\n","        else:\n","            plt.scatter(x=raw_data[node,0], y=raw_data[node,1], c='b', s=20, marker='+',\n","                        alpha=0.8)\n","    plt.title('The Result Of Cluster')\n","    plt.show()\n","\n","# Main dbscan method\n","def DBSCAN_main(distances, mds_data, epsilon, min_count):\n","    data_length = distances.shape[0]\n","    class_list = [-1 for _ in range(data_length)]\n","\n","    density_dict, core_point = get_core_point(distances, epsilon, min_count)\n","\n","    #print('Core points: ',len(core_point))\n","    noise_point = get_noise_point(density_dict, core_point, data_length)\n","    #print('Noise points: ',len(noise_point))\n","    class_list = assign_class(density_dict, core_point, data_length, class_list, noise_point)\n","    #print('Classes: ',len(class_list),set(class_list))\n","\n","    return class_list, core_point, len(noise_point)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"j0fELKAGMvuz","executionInfo":{"status":"ok","timestamp":1701238539881,"user_tz":420,"elapsed":181,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"}}},"outputs":[],"source":["# Display images thumbnails of the core points\n","def display_images_in_grid(label, core_points, columns=5, image_size=(2, 2), spacing=(0.2, 0.2)):\n","    rows = (len(core_points) + columns - 1) // columns\n","    if rows==1:\n","        columns = len(core_points)\n","    fig, axes = plt.subplots(rows, columns, figsize=(columns * image_size[0], rows * image_size[1]))\n","    plt.subplots_adjust(wspace=spacing[0], hspace=spacing[1])\n","\n","    for i, idx in enumerate(core_points):\n","      img = data[label_image_map[label][idx]][0]\n","\n","      if rows == 1:\n","        axes[i].imshow(img)\n","        axes[i].set_title(class_list[idx], fontsize=8)\n","        axes[i].axis('off')\n","      else:\n","        row, col = divmod(i, columns)\n","        axes[row, col].imshow(img)\n","        axes[row, col].set_title(class_list[idx], fontsize=8)\n","        axes[row, col].axis('off')\n","\n","    # Hide any remaining empty subplots\n","    for i in range(len(core_points), rows * columns):\n","      if rows == 1:\n","            axes[i].axis('off')\n","      else:\n","        row, col = divmod(i, columns)\n","        axes[row, col].axis('off')\n","\n","    plt.show()\n"]},{"cell_type":"code","source":["# Load the precalculated clusters and label DBSCAN parameters\n","def load_clusters(target_clusters):\n","    with open(f'{path}/clusters{target_clusters}.json','r') as fp:\n","        clusters = json.load(fp)\n","\n","    with open(f'{path}/labels_dbscan_data{target_clusters}.json','r') as fp:\n","        labels_dbscan_param = json.load(fp)\n","\n","    return clusters, labels_dbscan_param"],"metadata":{"id":"ymkz6p4CxdNc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T8ctmmvg9GEi"},"outputs":[],"source":["# for _ in core_point:\n","#     if _ ==-1:\n","#         continue\n","#     fig= plt.figure(figsize=(15, 5))\n","#     for ix,i in enumerate(class_list[_]):\n","#         fig.add_subplot(2, 6,ix+1)\n","#         plt.imshow(data[names[i]][0])\n","#         plt.title(f'cluster core:{_}')\n","#         plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5PXwR27c4_l"},"outputs":[],"source":["# New Image Label Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6SoDThFR3BE"},"outputs":[],"source":["# Get the Resnet50 output for an image by hooks in intermediate layers: layer3, avgpool, fc\n","def get_resnet(image):\n","\n","    # Load the model resnet50\n","    with torch.no_grad():\n","        m = resnet50(pretrained=True)\n","\n","        # Preprocess the image into 224x224 and to Tensor\n","        preprocess = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n","        tensor_image = preprocess(image)\n","\n","        if tensor_image.shape[0]==1:\n","            tensor_image = torch.cat([tensor_image,tensor_image,tensor_image], dim=0)\n","        outputs = {}\n","\n","        # Create hook function to get output of particular layers in resnet50\n","        def wrap_hook(name): # wrap the hook function to get layer name as key in the output dictionary\n","            def hook(module, input, output):\n","                outputs[name] = output\n","            return hook\n","\n","        # Layers to be hooked\n","        layers = {'layer3': m.layer3,'avgpool': m.avgpool,'fc': m.fc}\n","\n","        for name,layer in layers.items():\n","            layer.register_forward_hook(wrap_hook(name)) # Register the hook\n","\n","        # Pass the image in the pretrained model\n","        model = m(tensor_image.unsqueeze(0))\n","\n","        # Get the features and reshape to required dimensions\n","        features_avgpool = outputs['avgpool'].reshape((1024,2)).mean(dim=[1])\n","        features_layer3 = outputs['layer3'].reshape((1024,14,14)).mean(dim=[1,2])\n","        features_fc = outputs['fc'].reshape(-1)\n","\n","    return [features_avgpool,features_layer3,features_fc]\n"]},{"cell_type":"code","source":["# Transform the odd image to SVD latent space\n","def transform_odd_images_to_latent_space(latent_space, image_feature_vector):\n","    transformed_latent_space_vector = np.dot(np.array(image_feature_vector).reshape(1,-1), np.array(latent_space[\"V\"]))\n","    for i in range(transformed_latent_space_vector.shape[1]):\n","        transformed_latent_space_vector[:, i] /= latent_space[\"sigma\"][i]\n","    return transformed_latent_space_vector.squeeze()"],"metadata":{"id":"x2UBeeoZihFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SVD layer3 matrices: V, Sigma\n","with open(f\"{path}/latent_spaces/SVD_66_layer_3_latent_space.json\", \"r\") as fp:\n","    latent_space = json.load(fp)\n","\n","with open(f'{path}/classification_list.json','r') as fp:\n","    classification_list = json.load(fp)\n","\n","# Show the calculated classification metrics\n","def calculate_classification_metrics(actual_labels, predicted_labels):\n","    unique_labels = list(range(101))\n","    per_label_precision, per_label_recall, per_label_f1_score = calculate_per_label_metrics(actual_labels, predicted_labels)\n","    for label, precision, recall, f1 in zip(unique_labels, per_label_precision, per_label_recall, per_label_f1_score):\n","        print(f\"Label {label}: Precision={precision:.2f}, Recall={recall:.2f}, F1 Score={f1:.2f}\")\n","\n","    overall_accuracy = calculate_accuracy(actual_labels, predicted_labels)\n","    print(f\"Over all accuracy for the model is = {round(overall_accuracy*100,2)} %\")"],"metadata":{"id":"IrJlb0vc24kj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1sdQd2jozjo235c5b-es0btOOhsNKDTg7"},"executionInfo":{"elapsed":524022,"status":"ok","timestamp":1701239070356,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"6yCza_6nSFJY","outputId":"c8d70744-f466-4be2-dc6b-32087fc619c2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Main code to get clusters for labels and image thumbnails\n","target_clusters = int(input('Give target c clusters, 5 or 10: '))\n","clusters, labels_dbscan_param = load_clusters(target_clusters)\n","# For all Labels\n","for label in label_image_map:\n","    l = []\n","    for i in label_image_map[label]:\n","        l.append(latent_features[str(i)])\n","    l = np.array(l)\n","    print(f'For label {label}, feature matrix shape {l.shape}')\n","    min_count, epsilon = labels_dbscan_param[label]\n","\n","    mds_data, distances = MDS(l, 2)\n","\n","    # Main DBSCAN from scratch function distances, mincount_low, mincount_high, epsilon_low, epsilon_high, target_clusters, max_itr=100\n","    class_list, core_point, noise_cnt = DBSCAN_main(distances, mds_data, epsilon, min_count)\n","\n","    num_clusters = len(set(class_list)) - (1 if -1 in class_list else 0)\n","    print(f\"Saved values for label:{label}, epsilon:{epsilon}, mincount:{min_count}, Clusters:{num_clusters}, core points:{len(core_point)}\")\n","\n","    #use mds projected data in 2d to display the current clusters obtained via dbscan\n","    show_result(class_list, mds_data)\n","    display_images_in_grid(label, core_point)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4376,"status":"ok","timestamp":1701239399829,"user":{"displayName":"Devanshu Gupta","userId":"12117562172827221888"},"user_tz":420},"id":"Ekz4Nm_fR2-V","outputId":"3611de06-4b71-408b-fdec-aebc75ae5d9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Do you want 1. Saved file, 2. Calculate file again (Takes Time): 1\n","Label 0: Precision=0.93, Recall=0.99, F1 Score=0.96\n","Label 1: Precision=0.98, Recall=0.94, F1 Score=0.96\n","Label 2: Precision=0.88, Recall=1.00, F1 Score=0.94\n","Label 3: Precision=0.87, Recall=0.97, F1 Score=0.92\n","Label 4: Precision=0.68, Recall=0.85, F1 Score=0.75\n","Label 5: Precision=0.99, Recall=0.42, F1 Score=0.59\n","Label 6: Precision=0.08, Recall=0.10, F1 Score=0.09\n","Label 7: Precision=0.10, Recall=0.14, F1 Score=0.12\n","Label 8: Precision=1.00, Recall=0.50, F1 Score=0.67\n","Label 9: Precision=0.15, Recall=0.15, F1 Score=0.15\n","Label 10: Precision=0.20, Recall=0.22, F1 Score=0.21\n","Label 11: Precision=0.43, Recall=0.62, F1 Score=0.51\n","Label 12: Precision=0.83, Recall=0.69, F1 Score=0.75\n","Label 13: Precision=0.88, Recall=0.59, F1 Score=0.71\n","Label 14: Precision=0.29, Recall=0.23, F1 Score=0.26\n","Label 15: Precision=0.62, Recall=0.38, F1 Score=0.47\n","Label 16: Precision=0.56, Recall=0.30, F1 Score=0.39\n","Label 17: Precision=0.77, Recall=0.68, F1 Score=0.72\n","Label 18: Precision=0.06, Recall=0.29, F1 Score=0.10\n","Label 19: Precision=1.00, Recall=1.00, F1 Score=1.00\n","Label 20: Precision=0.62, Recall=0.43, F1 Score=0.51\n","Label 21: Precision=0.80, Recall=0.53, F1 Score=0.64\n","Label 22: Precision=0.25, Recall=0.16, F1 Score=0.20\n","Label 23: Precision=0.69, Recall=0.34, F1 Score=0.46\n","Label 24: Precision=0.18, Recall=0.46, F1 Score=0.26\n","Label 25: Precision=0.88, Recall=0.65, F1 Score=0.75\n","Label 26: Precision=0.06, Recall=0.27, F1 Score=0.10\n","Label 27: Precision=0.11, Recall=0.11, F1 Score=0.11\n","Label 28: Precision=0.04, Recall=0.08, F1 Score=0.05\n","Label 29: Precision=0.11, Recall=0.24, F1 Score=0.15\n","Label 30: Precision=0.71, Recall=0.41, F1 Score=0.52\n","Label 31: Precision=0.86, Recall=0.73, F1 Score=0.79\n","Label 32: Precision=0.79, Recall=0.73, F1 Score=0.76\n","Label 33: Precision=0.40, Recall=0.30, F1 Score=0.34\n","Label 34: Precision=0.28, Recall=0.50, F1 Score=0.36\n","Label 35: Precision=0.26, Recall=0.46, F1 Score=0.33\n","Label 36: Precision=0.42, Recall=0.31, F1 Score=0.36\n","Label 37: Precision=0.22, Recall=0.33, F1 Score=0.27\n","Label 38: Precision=0.67, Recall=0.62, F1 Score=0.65\n","Label 39: Precision=0.86, Recall=0.43, F1 Score=0.57\n","Label 40: Precision=0.76, Recall=0.56, F1 Score=0.64\n","Label 41: Precision=0.44, Recall=0.36, F1 Score=0.40\n","Label 42: Precision=0.40, Recall=0.35, F1 Score=0.37\n","Label 43: Precision=0.87, Recall=0.41, F1 Score=0.56\n","Label 44: Precision=0.06, Recall=0.18, F1 Score=0.09\n","Label 45: Precision=0.47, Recall=0.28, F1 Score=0.35\n","Label 46: Precision=1.00, Recall=0.72, F1 Score=0.84\n","Label 47: Precision=0.79, Recall=0.38, F1 Score=0.51\n","Label 48: Precision=0.52, Recall=0.52, F1 Score=0.52\n","Label 49: Precision=0.54, Recall=0.52, F1 Score=0.53\n","Label 50: Precision=0.08, Recall=0.25, F1 Score=0.12\n","Label 51: Precision=0.16, Recall=0.30, F1 Score=0.21\n","Label 52: Precision=0.52, Recall=0.87, F1 Score=0.65\n","Label 53: Precision=0.53, Recall=0.72, F1 Score=0.61\n","Label 54: Precision=0.41, Recall=0.37, F1 Score=0.39\n","Label 55: Precision=0.62, Recall=0.60, F1 Score=0.61\n","Label 56: Precision=0.68, Recall=0.42, F1 Score=0.52\n","Label 57: Precision=0.97, Recall=0.82, F1 Score=0.89\n","Label 58: Precision=0.18, Recall=0.23, F1 Score=0.20\n","Label 59: Precision=0.12, Recall=0.14, F1 Score=0.13\n","Label 60: Precision=0.39, Recall=0.48, F1 Score=0.43\n","Label 61: Precision=0.29, Recall=0.43, F1 Score=0.35\n","Label 62: Precision=0.13, Recall=0.15, F1 Score=0.14\n","Label 63: Precision=1.00, Recall=0.55, F1 Score=0.71\n","Label 64: Precision=0.78, Recall=0.87, F1 Score=0.82\n","Label 65: Precision=0.93, Recall=0.74, F1 Score=0.82\n","Label 66: Precision=1.00, Recall=0.37, F1 Score=0.54\n","Label 67: Precision=0.12, Recall=0.22, F1 Score=0.16\n","Label 68: Precision=0.50, Recall=0.42, F1 Score=0.46\n","Label 69: Precision=0.61, Recall=0.83, F1 Score=0.70\n","Label 70: Precision=0.62, Recall=0.42, F1 Score=0.50\n","Label 71: Precision=0.09, Recall=0.09, F1 Score=0.09\n","Label 72: Precision=0.82, Recall=0.67, F1 Score=0.73\n","Label 73: Precision=0.07, Recall=0.06, F1 Score=0.06\n","Label 74: Precision=0.58, Recall=0.39, F1 Score=0.47\n","Label 75: Precision=0.66, Recall=0.61, F1 Score=0.63\n","Label 76: Precision=0.11, Recall=0.13, F1 Score=0.12\n","Label 77: Precision=0.37, Recall=0.42, F1 Score=0.39\n","Label 78: Precision=0.16, Recall=0.35, F1 Score=0.22\n","Label 79: Precision=0.49, Recall=0.53, F1 Score=0.51\n","Label 80: Precision=0.62, Recall=0.42, F1 Score=0.50\n","Label 81: Precision=0.11, Recall=0.10, F1 Score=0.10\n","Label 82: Precision=0.15, Recall=0.21, F1 Score=0.18\n","Label 83: Precision=0.61, Recall=0.65, F1 Score=0.63\n","Label 84: Precision=0.94, Recall=0.53, F1 Score=0.68\n","Label 85: Precision=0.30, Recall=0.13, F1 Score=0.18\n","Label 86: Precision=0.30, Recall=0.14, F1 Score=0.19\n","Label 87: Precision=0.41, Recall=0.45, F1 Score=0.43\n","Label 88: Precision=1.00, Recall=0.87, F1 Score=0.93\n","Label 89: Precision=0.76, Recall=0.72, F1 Score=0.74\n","Label 90: Precision=0.97, Recall=0.74, F1 Score=0.84\n","Label 91: Precision=0.65, Recall=0.60, F1 Score=0.62\n","Label 92: Precision=0.74, Recall=0.91, F1 Score=0.81\n","Label 93: Precision=0.70, Recall=0.38, F1 Score=0.49\n","Label 94: Precision=1.00, Recall=0.52, F1 Score=0.69\n","Label 95: Precision=0.26, Recall=0.50, F1 Score=0.34\n","Label 96: Precision=0.44, Recall=0.40, F1 Score=0.42\n","Label 97: Precision=0.21, Recall=0.24, F1 Score=0.22\n","Label 98: Precision=1.00, Recall=0.68, F1 Score=0.81\n","Label 99: Precision=0.47, Recall=0.37, F1 Score=0.41\n","Label 100: Precision=0.94, Recall=0.53, F1 Score=0.68\n","Over all accuracy for the model is = 56.89 %\n"]}],"source":["# Main code to get label predictions for Odd Image\n","# This code will calculate the resnet features for odd images and find distance of the image to core points of the labels, and pick one most likely label\n","if input('Do you want 1. Saved file, 2. Calculate file again (Takes Time): ') == '2':\n","    classification_list = {}\n","    for image_id in range(1,8677,2):\n","        input_image = image_id\n","        image = data[input_image][0]\n","\n","        image_avgpool,image_layer3,image_fc = get_resnet(image)\n","        image_layer3 = np.array(image_layer3)\n","\n","        image_latent_features = transform_odd_images_to_latent_space(latent_space, image_layer3)\n","        labels = []\n","        for label in label_image_map:\n","            epsilon = labels_dbscan_param[label][1]\n","            core_points = clusters[label]['core']\n","            connected = 0\n","            min_dist = 100\n","            for core in core_points:\n","                a = image_latent_features\n","                b = latent_features[str(core)]\n","                dist = euclidean(a,b)\n","                if dist<=epsilon:\n","                    connected+=1\n","                min_dist = min(dist,min_dist)\n","            labels.append((label,min_dist,connected))\n","        actual_label = data[image_id][1]\n","        labels = sorted(labels, key = lambda x: x[1])\n","        classification_list[image_id] = [int(labels[0][0]), actual_label]\n","    with open(f'{path}/classification_list.json','w') as fp:\n","        json.dump(classification_list,fp)\n","\n","actual_labels, predicted_labels = [], []\n","for i in classification_list:\n","    lab,alab = classification_list[i]\n","    predicted_labels.append(lab)\n","    actual_labels.append(alab)\n","# Calculate and show the classification metrics\n","calculate_classification_metrics(actual_labels, predicted_labels)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"L3kEsB_jG2Jw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yh1hgRHrG2Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gAyID_RAG2Wi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CfbrWZaTG2ik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gEH5fUO_G2BB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f3FjnNNTQoOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fO6RcRqt3ZmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OyvFS5ygbeNx"},"outputs":[],"source":["'''\n","def save_dbscan_results(clusters, dbscan_data,target_clusters):\n","  # Save Clusters into json\n","  with open(f'{path}/clusters{target_clusters}.json','w') as fp:\n","      json.dump(clusters,fp)\n","  # Save MinCount Epsilon into json\n","  with open(f'{path}/labels_dbscan_data{target_clusters}.json','w') as fp:\n","      json.dump(dbscan_data,fp)\n","\n","def binary_search_clusters(distances, mincount_low, mincount_high, epsilon_low, epsilon_high, target_clusters, max_itr=500):\n","  best_noise = distances.shape[0]\n","  best_clusters = 0\n","  while mincount_low <= mincount_high:\n","    itr = 0\n","    ep_l = epsilon_low\n","    ep_h = epsilon_high\n","    while itr<=max_itr:\n","        mid_epsilon = (epsilon_low + epsilon_high) / 2\n","        #print(f\"Epsilon:{mid_epsilon}, mincount:{mincount_high}\")\n","        class_list, core_point, noise_cnt = DBSCAN_main(distances, mds_data, epsilon = mid_epsilon, min_count = mincount_high)\n","        num_clusters = len(set(class_list)) - (1 if -1 in class_list else 0)\n","        itr+=1\n","        if num_clusters==target_clusters:\n","            print('best found at :',mincount_high, mid_epsilon)\n","        if num_clusters > best_clusters:\n","            best_clusters = num_clusters\n","            best_noise = noise_cnt\n","            best_eps = mid_epsilon\n","            best_mincount = mincount_high\n","        if num_clusters==0 or num_clusters > target_clusters or len(core_point) < target_clusters:\n","            epsilon_low = mid_epsilon\n","        else:\n","            epsilon_high = mid_epsilon - 1e-6\n","\n","    # Reset epsilon range for the next iteration\n","    epsilon_low, epsilon_high = ep_l, ep_h\n","    mincount_high -= 1\n","\n","  # # If no exact match is found, return the closest values\n","  # show_result(class_list, mds_data)\n","  print('noise: ',best_noise)\n","  return best_mincount, best_eps, class_list, core_point\n","'''"]},{"cell_type":"code","source":["'''target_clusters = int(input('Give target c clusters: '))\n","# For all Labels\n","for label in label_image_map:\n","    l = []\n","    for i in label_image_map[label]:\n","        l.append(latent_features[str(i)])\n","    l = np.array(l)\n","    print(f'For label {label}, feature matrix shape {l.shape}')\n","\n","    mds_data, distances = MDS(l, 2)\n","\n","    #use mds projected data in 2d to display the current clusters obtained via dbscan\n","    show_result(class_list, mds_data)\n","\n","    # Store MinPoint, Epsilon values and scale\n","    labels_dbscan_param[label] = [min_count, epsilon]\n","\n","    # Get Clusters from class list and store\n","    clusters[label] = {'core':[], 'border':[]}\n","\n","    for point, class_ in enumerate(class_list):\n","      if point in core_point:\n","        clusters[label]['core'].append(label_image_map[str(label)][point])\n","      elif class_list != -1:\n","        clusters[label]['border'].append(label_image_map[str(label)][point])\n","\n","save_dbscan_results(clusters, labels_dbscan_param,target_clusters)'''"],"metadata":{"id":"7-Y_LFN6wxP7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}