{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLg0zngbcHo_"
      },
      "source": [
        "Task 1: Implement a program which,\n",
        "\n",
        "*   for each unique label l, computes the corresponding k latent semantics (of your choice) associated with the even\n",
        "numbered Caltec101 images, and\n",
        "*   for the odd numbered images, predicts the most likely labels using distances/similarities computed under the\n",
        "label-specific latent semantics.\n",
        "\n",
        "The system should also output per-label precision, recall, and F1-score values as well as output an overall accuracy\n",
        "value.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF-K_m4zFsty",
        "outputId": "6b1effbc-ef8c-4749-838c-8e0939cbacb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7SoM8AqlcFwd"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import Caltech101\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UtWnri-fGFUl"
      },
      "outputs": [],
      "source": [
        "def load_json(file_path):\n",
        "  with open(f\"/content/drive/MyDrive/CSE515_Phase3/{file_path}\",\"r\") as f:\n",
        "    data = json.load(f)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-YqpinqnGsqb"
      },
      "outputs": [],
      "source": [
        "caltech_dataset = Caltech101(\"/content/drive/MyDrive/CSE515_Phase3/data\", download=False)\n",
        "\n",
        "features = load_json(\"feature_descriptors.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "odd_image_features = load_json(\"feature_descriptors_odd_images.json\")"
      ],
      "metadata": {
        "id": "EWAf39J-yQxp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_labels = [caltech_dataset.y[i] for i in range(1,8677,2)]"
      ],
      "metadata": {
        "id": "5st2F_dQ6M1p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dZMptpGpGi5q"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(features).T\n",
        "data = data[['label','avg_pool','layer_3']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x83_hs7Mcbba"
      },
      "outputs": [],
      "source": [
        "def get_svd_latent_features(k, feature_vector, data ):\n",
        "\n",
        "    feature_vectors = np.vstack(data[feature_vector].to_numpy())\n",
        "\n",
        "    ATA = np.dot(feature_vectors.T, feature_vectors)\n",
        "\n",
        "\n",
        "    #Calculates eigenvalues and eigenvectors of an already symmetric matrix (eigenvectors give us V matrix)\n",
        "    eigenvalues_ATA, eigenvectors_ATA = np.linalg.eigh(ATA)\n",
        "\n",
        "    sorted_indices = eigenvalues_ATA.argsort()[::-1]\n",
        "    eigenvalues_ATA = eigenvalues_ATA[sorted_indices]\n",
        "    eigenvectors_ATA = eigenvectors_ATA[:, sorted_indices]\n",
        "\n",
        "    singular_values = np.sqrt(eigenvalues_ATA)\n",
        "    right_singular_vectors = eigenvectors_ATA\n",
        "\n",
        "    # The below code will help us compute the U matrix which is the latent space that we will be using\n",
        "    left_singular_vectors = np.dot(feature_vectors, right_singular_vectors)\n",
        "    for i in range(left_singular_vectors.shape[1]):\n",
        "        left_singular_vectors[:, i] /= singular_values[i]\n",
        "\n",
        "    right_singular_vectors = right_singular_vectors[:,:k]\n",
        "\n",
        "    latent_weights = left_singular_vectors\n",
        "\n",
        "\n",
        "    svd_weights = {}\n",
        "    for i in range(len(latent_weights)):\n",
        "        svd_weights[i*2] = np.real(latent_weights[i][:k]).tolist()\n",
        "\n",
        "    # V and sigma matrices are stored to later use in the latent projection of odd images\n",
        "    return svd_weights,{\"V\":np.real(right_singular_vectors).tolist(),\"sigma\":np.real(singular_values).tolist(), \"k\":k}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_per_label_metrics(actual_labels, predicted_labels):\n",
        "    unique_labels = set(actual_labels + predicted_labels)\n",
        "    label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "    label_count = len(unique_labels)\n",
        "\n",
        "    true_positives = [0] * label_count\n",
        "    false_positives = [0] * label_count\n",
        "    false_negatives = [0] * label_count\n",
        "\n",
        "    for actual, predicted in zip(actual_labels, predicted_labels):\n",
        "        actual_index = label_to_index[actual]\n",
        "        predicted_index = label_to_index[predicted]\n",
        "\n",
        "        if actual == predicted:\n",
        "            true_positives[actual_index] += 1\n",
        "        else:\n",
        "            false_positives[predicted_index] += 1\n",
        "            false_negatives[actual_index] += 1\n",
        "\n",
        "    precision = [true_positives[i] / (true_positives[i] + false_positives[i] + 1e-10) for i in range(label_count)]\n",
        "    recall = [true_positives[i] / (true_positives[i] + false_negatives[i] + 1e-10) for i in range(label_count)]\n",
        "    f1_score = [2 * (p * r) / (p + r + 1e-10) for p, r in zip(precision, recall)]\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "def calculate_accuracy(actual_labels, predicted_labels):\n",
        "    correct_predictions = sum(1 for a, p in zip(actual_labels, predicted_labels) if a == p)\n",
        "    accuracy = correct_predictions / len(actual_labels)\n",
        "    return accuracy\n",
        "\n",
        "def calculate_classification_metrics(actual_labels, predicted_labels):\n",
        "  unique_labels = list(range(101))\n",
        "  per_label_precision, per_label_recall, per_label_f1_score = calculate_per_label_metrics(actual_labels, predicted_labels)\n",
        "  for label, precision, recall, f1 in zip(unique_labels, per_label_precision, per_label_recall, per_label_f1_score):\n",
        "      print(f\"Label {label}: Precision={precision:.2f}, Recall={recall:.2f}, F1 Score={f1:.2f}\")\n",
        "\n",
        "  overall_accuracy = calculate_accuracy(actual_labels, predicted_labels)\n",
        "\n",
        "  print(f\"Over all accuracy for the model is = {round(overall_accuracy*100,2)} %\")\n"
      ],
      "metadata": {
        "id": "EXKWZ5pN23l-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_odd_images_to_latent_space_svd(latent_space, image_feature_vector):\n",
        "    \"\"\"\n",
        "    Transforms a new image to latent space by using the V and sigma matrices stored during the even images svd process\n",
        "    \"\"\"\n",
        "    transformed_latent_space_vector = np.dot(np.array(image_feature_vector).reshape(1,-1), np.array(latent_space[\"V\"]))\n",
        "    for i in range(transformed_latent_space_vector.shape[1]):\n",
        "        transformed_latent_space_vector[:, i] /= latent_space[\"sigma\"][i]\n",
        "    return transformed_latent_space_vector.squeeze()"
      ],
      "metadata": {
        "id": "vd9CfV9U4xf7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_for_odd_images_comb( odd_image_features, latent_feature_space, inherent_dim, mean_label_latent_vector,actual_labels):\n",
        "    \"\"\"\n",
        "    Transforms the images to the latent space and compares with all the mean label latent vectors to get the most appropriate label\n",
        "    \"\"\"\n",
        "    predicted_labels = []\n",
        "    i = 0\n",
        "    for _,odd_feature in odd_image_features.items():\n",
        "        latent_vector = transform_odd_images_to_latent_space_svd(latent_feature_space, odd_feature)\n",
        "        similarities = cosine_similarity([latent_vector], mean_label_latent_vector)[0]\n",
        "        sorted_indices = np.argsort(similarities)[::-1]\n",
        "        image_label = sorted_indices[0]\n",
        "        predicted_labels.append(int(sorted_indices[0]))\n",
        "        i += 1\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "VxOwuAE3xZIA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label_for_odd_images_label_wise( odd_image_features, latent_feature_space, inherent_dim, mean_label_latent_vector,actual_labels):\n",
        "    \"\"\"\n",
        "    Transforms the images to the label wise latent space and compares with the respective mean label latent vectors to get the most appropriate label\n",
        "    \"\"\"\n",
        "    predicted_labels = []\n",
        "    i = 0\n",
        "    for _,odd_feature in odd_image_features.items():\n",
        "        latent_vectors = []\n",
        "        similarities = []\n",
        "        for idx,dim in enumerate(inherent_dim):\n",
        "            latent_vector = transform_odd_images_to_latent_space_svd(latent_feature_space[idx], odd_feature)\n",
        "            similarities.append(cosine_similarity([np.array(latent_vector).round(3)], [np.array(mean_label_latent_vector[idx]).round(3)])[0][0])\n",
        "        sorted_indices = np.argsort(similarities)[::-1]\n",
        "        predicted_labels.append(int(sorted_indices[0]))\n",
        "        i += 1\n",
        "    return predicted_labels"
      ],
      "metadata": {
        "id": "WCHahRXHx-8i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_latent_semantics(k, semantics, data, odd_image_features, actual_labels):\n",
        "  feature_space = \"layer_3\"\n",
        "  inherent_dim = [k for _ in range(101)]\n",
        "\n",
        "  if semantics == \"combined\":\n",
        "    weights, latent_feature_space = get_svd_latent_features(k,feature_space,data)\n",
        "\n",
        "    label_wise_latent_features = {i:[] for i in range(101)}\n",
        "\n",
        "    for image_id in weights:\n",
        "        corr_label = caltech_dataset.y[int(image_id)]\n",
        "        label_wise_latent_features[corr_label].append(weights[image_id][:inherent_dim[corr_label]])\n",
        "\n",
        "    mean_label_latent_vector = [np.mean([value for value in label_wise_latent_features[i]],axis=0) for i in range(101)]\n",
        "\n",
        "    pred_labels = get_label_for_odd_images_comb(odd_image_features, latent_feature_space, inherent_dim, mean_label_latent_vector, actual_labels)\n",
        "\n",
        "  else:\n",
        "    data_gb = data.groupby('label')\n",
        "    label_groups = [data_gb.get_group(x) for x in data_gb.groups]\n",
        "\n",
        "    label_latent_weights = []\n",
        "    latent_feature_space = []\n",
        "\n",
        "    for idx,k in enumerate(inherent_dim):\n",
        "        weights, latent_feature = get_svd_latent_features(k,feature_space,label_groups[idx])\n",
        "        latent_feature_space.append(latent_feature)\n",
        "        label_latent_weights.append(weights)\n",
        "\n",
        "    mean_label_latent_vector = [np.mean([value for key,value in label_latent_weights[i].items()],axis=0) for i in range(101)]\n",
        "\n",
        "    pred_labels = get_label_for_odd_images_label_wise(odd_image_features, latent_feature_space, inherent_dim, mean_label_latent_vector, actual_labels)\n",
        "\n",
        "  with open(f\"/content/drive/MyDrive/CSE515_Phase3/svd_{feature_space}_{k}_task_1_{semantics}_predicted_labels.json\",\"w\") as f:\n",
        "    json.dump(pred_labels,f)\n",
        "\n",
        "  return pred_labels"
      ],
      "metadata": {
        "id": "jIaNwfwKwV2h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label_classes(k, choice, data, odd_image_features, actual_labels, semantics):\n",
        "  if choice == 1:\n",
        "    feature_space = \"layer_3\"\n",
        "    if semantics  == 1:\n",
        "        semantics = \"comb\"\n",
        "    else:\n",
        "      semantics = \"sep\"\n",
        "    pred_labels = load_json(f\"svd_{feature_space}_{k}_task_1_{semantics}_predicted_labels.json\")\n",
        "  else:\n",
        "    pred_labels = calculate_latent_semantics(k, semantics, data, odd_image_features, actual_labels)\n",
        "\n",
        "  calculate_classification_metrics(actual_labels, pred_labels)"
      ],
      "metadata": {
        "id": "WFBXfdKdvbNU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = int(input('Enter the value of k for latent semantics : '))\n",
        "choice = int(input('Choose one: 1. saved data 2. calculate latent semantics (LONGER TIME) : '))\n",
        "semantics = int(input(\"Compute under 1. combined latent semantics 2. label wise latent semantics : \"))\n",
        "predict_label_classes(k, choice, data, odd_image_features, actual_labels,semantics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncqutk7XvMlY",
        "outputId": "c915bf30-742e-42e1-ac27-6f2ab37d037b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the value of k for latent semantics : 5\n",
            "Choose one: 1. saved data 2. calculate latent semantics (LONGER TIME) : 1\n",
            "Compute under 1. combined latent semantics 2. label wise latent semantics2\n",
            "Label 0: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 1: Precision=0.33, Recall=0.00, F1 Score=0.01\n",
            "Label 2: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 3: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 4: Precision=0.02, Recall=0.04, F1 Score=0.02\n",
            "Label 5: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 6: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 7: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 8: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 9: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 10: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 11: Precision=0.00, Recall=0.06, F1 Score=0.01\n",
            "Label 12: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 13: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 14: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 15: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 16: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 17: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 18: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 19: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 20: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 21: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 22: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 23: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 24: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 25: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 26: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 27: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 28: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 29: Precision=0.01, Recall=0.04, F1 Score=0.01\n",
            "Label 30: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 31: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 32: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 33: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 34: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 35: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 36: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 37: Precision=0.12, Recall=0.07, F1 Score=0.09\n",
            "Label 38: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 39: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 40: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 41: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 42: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 43: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 44: Precision=0.01, Recall=0.29, F1 Score=0.02\n",
            "Label 45: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 46: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 47: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 48: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 49: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 50: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 51: Precision=0.11, Recall=0.05, F1 Score=0.07\n",
            "Label 52: Precision=0.01, Recall=0.07, F1 Score=0.01\n",
            "Label 53: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 54: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 55: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 56: Precision=0.06, Recall=0.03, F1 Score=0.04\n",
            "Label 57: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 58: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 59: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 60: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 61: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 62: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 63: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 64: Precision=0.01, Recall=0.06, F1 Score=0.02\n",
            "Label 65: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 66: Precision=0.15, Recall=0.07, F1 Score=0.10\n",
            "Label 67: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 68: Precision=0.02, Recall=0.11, F1 Score=0.03\n",
            "Label 69: Precision=0.29, Recall=0.08, F1 Score=0.13\n",
            "Label 70: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 71: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 72: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 73: Precision=0.01, Recall=0.06, F1 Score=0.02\n",
            "Label 74: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 75: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 76: Precision=0.03, Recall=0.03, F1 Score=0.03\n",
            "Label 77: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 78: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 79: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 80: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 81: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 82: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 83: Precision=0.01, Recall=0.06, F1 Score=0.01\n",
            "Label 84: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 85: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 86: Precision=0.02, Recall=0.02, F1 Score=0.02\n",
            "Label 87: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 88: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 89: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 90: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 91: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 92: Precision=0.17, Recall=0.02, F1 Score=0.04\n",
            "Label 93: Precision=0.08, Recall=0.03, F1 Score=0.04\n",
            "Label 94: Precision=0.50, Recall=0.01, F1 Score=0.02\n",
            "Label 95: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 96: Precision=0.10, Recall=0.03, F1 Score=0.05\n",
            "Label 97: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 98: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Label 99: Precision=0.04, Recall=0.16, F1 Score=0.06\n",
            "Label 100: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
            "Over all accuracy for the model is = 0.76 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNPG4GSD6X07"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}