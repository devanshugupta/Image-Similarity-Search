# -*- coding: utf-8 -*-
"""Phase2_Task0_a.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KgTZfV1MayMWkuCxnfnuW2XT1QklLXbb
"""

#final code as in local

#Imports needed to run the code
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
from torchvision import models, transforms
from scipy.signal import convolve2d
import json
import matplotlib.pyplot as plt
import json
from scipy.special import softmax

# dataset
dataset = torchvision.datasets.Caltech101(r'../Dataset', download = True) # Caltech101 Dataset will be downloaded in the mentioned path, if already downloaded it will not download again

# JSON file paths
fd_json_file_path = '../Feature_Descriptors.json'     # path of the file used to store the data
label_image_mapping_json_file_path = '../LabelsWithImages.json'  #path of the file used to store the label to image mapping


# Function to save the data to a JSON file. If the file is not available it will create a file in the root project
def save_data_to_json(data, json_file):
    with open(json_file, 'w') as file:
        json.dump(data, file, indent=4)


# Function to load the existing data from a JSON file
def load_data_from_json(json_file):
    try:
        with open(json_file, 'r') as file:
            data = json.load(file)
    except FileNotFoundError:
        data = {}  # Initialize with an empty dictionary if the file doesn't exist
    return data


#Function defined to find Mean
def findMean(cell_array):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + cell_array[i][j]
    N = len(cell_array)*len(cell_array[0])
    return sum/N


#Function defined to find Standard deviation using previously calculated Mean
def findStandardDeviation(cell_array, mean):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),2)
    N = len(cell_array)*len(cell_array[0])
    return np.float_power((sum/N),1/2.)


#Function defined to find Skewness using previously calculated Standard Deviation
def findSkewness(cell_array , mean):
    sum = 0.0

    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),3)
    N = len(cell_array)*len(cell_array[0])
    #Using a Negative indicator so that skewness can support Negative values
    neg = False
    if sum < 0:
        neg = True
        sum = sum * -1
    skew = np.float_power((sum/N),1/3.)
    if neg:
        skew = -1 * skew
    return skew



#Function defined to Compute Color Moments for an Image
#Here image_id represents the id of the image in the dataset which is passed as 'dataset'
def computeColorMoments(image_id, dataset):
    img, label = dataset[image_id]            #Retrieving the image from the dataset by image_id

    #Resizing the image so that every image is scaled to the same size for Feature Descriptor extraction
    #The image is resized to a image of width 300 and height 100 as per the given problem description
    resized_img = img.resize((300,100))
    img_array = np.array(resized_img)         #Converting the image to numpy array to perform mathematical operations

    if (len(img_array.shape) != 3 or img_array.shape[2] != 3):           #Skipping computation of images which doesn't have all 3 colour channels
        img_array = np.stack((img_array,) * 3, axis=-1)

    grid_size = (10,10)                       #Partitioning the image with a grid of size (10,10) which is defined in the problem description
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])     #The cell size calculated based on the grid used for partition
    color_moments = []                        #Intialising an empty array to store the computed color Moments for each cell in the Image
    for i in range(10):
        for j in range(10):

            #Calculating the height and width of each cell for which the color Moments are calculated
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell = img_array[h1:h2, w1:w2]              #Slicing the intial image array to include only the cell data which can be used for color Moments computation
            moments = []
            for color_channel in range(3):
                channel_values = cell[:,:,color_channel]               #Dividing the array based on color channels R, G, B

                #Mean, Standard Deviation and Skewness Calculations using the custom defined functions for each color channel
                mean = findMean(channel_values)
                standard_deviation = findStandardDeviation(channel_values, mean)
                skewness = findSkewness(channel_values , mean)

                #Adding the color Moments of each color channel to a Temporary array
                moments.append(mean)
                moments.append(standard_deviation)
                moments.append(skewness)

            #Adding the computed color Moments of each cell to the final array
            color_moments.append(moments)

    color_moments = np.array(color_moments)
    color_moments_feature_descriptor = color_moments.reshape(10,10,3,3).flatten()       #Reshaping the computed colorMoments to (1, 900) shape for better understanding. Here 10,10 represents the cells for which color Moments are calculated and in the 3,3 matrix the row represents the colors and columns represent the mean, standard deviation and skewness values
    return color_moments_feature_descriptor


#Function defined to compute Histogram of Oriented Gradients
def computeHOG(imageId, dataset):
    img, label = dataset[imageId]
    img = img.convert("L")                            #Converting the image to gray scale as we are more focussed on the orientation than colors
    resized_image = img.resize((300,100))
    img_array = np.array(resized_image)
    grid_size = (10,10)                               #Partitioning the image with a 10,10 grid
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])
    dx_mask = np.array([[-1, 0, 1]])                  # Filter Mask we use to convolve the image array for horizantal gradient calculation
    dy_mask = dx_mask.T                               # Transposing the horizantal Filter Mask to convolve the image array for vertical gradient calculation
    grad_x = convolve2d(img_array, dx_mask, mode='same')         # X gradient calculation
    grad_y = convolve2d(img_array, dy_mask, mode='same')         # Y gradient calculation
    magnitude = np.sqrt(grad_x**2 + grad_y**2)              # Using Pythogrus formula to calculate the magnitude of the gradients
    orientation = np.arctan2(grad_y, grad_x)*180/np.pi      # Calculating the orientation of the gradients and converting it to degrees
    orientation[orientation < 0] += 360                     # Converting the negative orientation to be in the range of 0 to 360
    num_bins = 9                                            # Splitting the orientation into 9 bins each bins composing 40 degrees
    hog_features = np.zeros((10, 10, 9))
    for i in range(10):
        for j in range(10):
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell_orientation = orientation[h1:h2, w1:w2]
            cell_magnitude = magnitude[h1:h2, w1:w2]
            hist, bin_edges = np.histogram(cell_orientation, bins=num_bins, range=(0, 360), weights = cell_magnitude)       # Mapping a magintude based histogram for each cell with 9 bins based on their orientation
            hog_features[i, j, :] = hist

    hog_feature_descriptors = hog_features.flatten()                # Converting the HOG feature descriptors to 1D from 10, 10, 9 to store in JSON file
    return hog_feature_descriptors


#Function defined to retrieve the vectors from different layers of the pre-trained neural architecture : ResNet50
def computeResNet50Vectors(imageId, dataset):
    image, label = dataset[imageId]
    if (image.mode == 'L') :
      image = image.convert("RGB")
    resnet = models.resnet50(pretrained=True)          # Loading the ResNet50 model
    hook_output_avg_pool = []                          # list to store the ResNet50 avg pool layer output
    hook_output_layer3 = []                            # list to store the ResNet50 layer-3 layer output
    hook_output_fc = []                                # list to store the ResNet50 Full Connected layer output


    # Hook defined to capture the feature vectors from the avg_pool layer
    def hook_fn(module, input, output):
        hook_output_avg_pool.append(output)

    # Hook defined to capture the feature vectors from the Layer3
    def hook_fn_layer3(module, input, output):
        hook_output_layer3.append(output)

    # Hook defined to capture the feature vectors from the Fully Connected layer
    def hook_fn_fc(module, input, output):
        hook_output_fc.append(output)


    # Registering the defined hook as the forward hook on the avg pool layer on the ResNet 50 model
    avgpool_layer = resnet.avgpool
    avgpool_layer.register_forward_hook(hook_fn)

    # Registering the defined hook as the forward hook on the layer-3 on the ResNet 50 model
    layer3 = resnet.layer3
    layer3.register_forward_hook(hook_fn_layer3)

    # Registering the defined hook as the forward hook on the fully connected layer on the ResNet 50 model
    fc_layer = resnet.fc
    fc_layer.register_forward_hook(hook_fn_fc)

    # Performing tranform operations on the image to resize it and retrieve the tensors from each layer
    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])
    image = transform(image).unsqueeze(0)
    resnet(image)                # Passing the image to the ResNet Model


    #Storing the output of the hooks in the predefined lists
    avgpool_vector = hook_output_avg_pool[0][0]
    layer3_vector = hook_output_layer3[0][0]
    fc_vector = hook_output_fc[0][0]


    avgpool_vector = avgpool_vector.squeeze().detach().numpy().reshape(1024, 2)           # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_avg_pool_dimensionality_vector = np.mean(avgpool_vector, axis = 1)            # Dimensional Reduction is perfromed by averaging up the consecutive numbers and reducing the resultant array to 1D

    layer3_vector = layer3_vector.detach().numpy()                                        # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_layer3_dimensionality_vector = np.mean(layer3_vector, axis = (1, 2))          # Dimensional Reduction is perfromed by averaging up the 14, 14 slice and reducing the resultant array to 1D

    fc_vector = fc_vector.detach().numpy()                                                            # Converting the vectors from tensor to numpy to perform mathematical operations
    resnet = softmax(fc_vector)
    return (reduced_avg_pool_dimensionality_vector, reduced_layer3_dimensionality_vector, fc_vector, resnet)  # returning the feature descriptors of all 3 layers in a tuple



def computeFeatureDescriptors(dataset):
  data = {}
  labelsWithImages = {}

  for image_id in range(len(dataset)):
    if image_id % 2 == 0:                                               # Considering only images with even ids for training purposes
      img, label = dataset[image_id]
      print("Calculating Feature Descriptors for ", image_id)            # Just a print statement to show that program is computing descriptors for the particular image

      #Computation of Feature Descriptors
      colorMomentFD = computeColorMoments(image_id, dataset)
      HOGFeatureDescriptors = computeHOG(image_id, dataset)
      ResNet_output_vectors = computeResNet50Vectors(image_id, dataset)

      # Code to save feature descriptors in the Database
      data[image_id] = {
          'label' : label,
          'color_moments': colorMomentFD.tolist(),
          'hog':    HOGFeatureDescriptors.tolist(),
          'avg_pool': ResNet_output_vectors[0].tolist(),
          'layer_3': ResNet_output_vectors[1].tolist(),
          'fc': ResNet_output_vectors[2].tolist(),
          'resnet' : ResNet_output_vectors[3].tolist()
      }

      # Code to save labels and images under each label in the dataset
      if label not in labelsWithImages:
        labelsWithImages[label] = []
      labelsWithImages[label].append(image_id)



  # Saving the updated data to the JSON file
  save_data_to_json(data, fd_json_file_path)
  save_data_to_json(labelsWithImages, label_image_mapping_json_file_path)
  print("data saved successfully")



#Loading features data from existing file or intializing data with empty dictionary
data = load_data_from_json(fd_json_file_path)
exit = False
while (not exit) :
  choice = int(input("Enter your choice 1. to compute and store feature descriptors 2. to visualize computed feature descriptors for a image_id\n"))   # User Input choice to choose any of the functions. The choice 1. computes the feature descriptors and stores them in a JSON file. The choice 2 retrieves the existing data from the JSON and displays it in a nice way
  if choice == 2:
    input_image = int(input("Enter the imageId for which you want to visualize the feature descriptors\n"))
    try :
      if (int(input_image) % 2 == 0) :     # Checking whether the image is in the dataset or not and whether it is part of our training dataset
        image_id = int(input_image)
        input_image, label = dataset[image_id]
        if (input_image is None) :          # Case where we couldn't find the input image
          print("we couldn't find an image associated with the given image_id in the caltech 101 dataset ")
          continue
        vector_space = int(input("Enter the Vector space you want to select. The choice is as follows \n 1. color_moments 2. hog 3. avg_pool 4. layer_3 5. fc 6. resnet softmax \n"))
        print("The corresponding input image is\n")
        plt.imshow(input_image)             # Displaying the given ImageId
        plt.show()

        # Case where the feature descriptors are not saved in the database
        if len(data) == 0 :
          print("The feature descriptors are not stored please choose 1 to compute the feature descriptors and store them")
          continue

        # Based on the vector space retrieving the feature descriptors and visualizing them in a human readable format
        if vector_space == 1:
          vector_space_name = 'color_moments'
          fd_descriptor = np.array(data[str(image_id)][vector_space_name]).reshape(10,10,3,3)
          print(fd_descriptor)
        elif vector_space == 2:
          vector_space_name = 'hog'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 3:
          vector_space_name = 'avg_pool'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 4:
          vector_space_name = 'layer_3'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 5:
          vector_space_name = 'fc'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == '6':
           vector_space_name = 'resnet'
           fd_descriptor = data[str(image_id)][vector_space_name]
           print(fd_descriptor)
        else:
          print("The vector space you selected is not one of the allowed options. Please check again")
          continue
    except Exception as e:
      print(f"An error occurred: {e}")
  else:                     #If choice is anything other than 2 we go for computation of feature descriptors
    computeFeatureDescriptors(dataset)

  exit_choice = int(input("For Menu enter 1, To exit enter 2 \n"))
  if (exit_choice != 1):
        exit = True







from google.colab import drive
drive.mount('/content/drive')

#Imports needed to run the code
import torchvision
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
from torchvision import models, transforms
from scipy.signal import convolve2d
import json
import matplotlib.pyplot as plt
import json

from scipy.special import softmax

# dataset
dataset = torchvision.datasets.Caltech101(r'../Dataset', download = True) # Caltech101 Dataset will be downloaded in the mentioned path, if already downloaded it will not download again

# JSON file paths
fd_json_file_path = '../Feature_Descriptors.json'     # path of the file used to store the data
label_image_mapping_json_file_path = '../LabelsWithImages.json'  #path of the file used to store the label to image mapping


# Function to save the data to a JSON file. If the file is not available it will create a file in the root project
def save_data_to_json(data, json_file):
    with open(json_file, 'w') as file:
        json.dump(data, file, indent=4)


# Function to load the existing data from a JSON file
def load_data_from_json(json_file):
    try:
        with open(json_file, 'r') as file:
            data = json.load(file)
    except FileNotFoundError:
        data = {}  # Initialize with an empty dictionary if the file doesn't exist
    return data


#Function defined to find Mean
def findMean(cell_array):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + cell_array[i][j]
    N = len(cell_array)*len(cell_array[0])
    return sum/N


#Function defined to find Standard deviation using previously calculated Mean
def findStandardDeviation(cell_array, mean):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),2)
    N = len(cell_array)*len(cell_array[0])
    return np.float_power((sum/N),1/2.)


#Function defined to find Skewness using previously calculated Standard Deviation
def findSkewness(cell_array , mean):
    sum = 0.0

    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),3)
    N = len(cell_array)*len(cell_array[0])
    #Using a Negative indicator so that skewness can support Negative values
    neg = False
    if sum < 0:
        neg = True
        sum = sum * -1
    skew = np.float_power((sum/N),1/3.)
    if neg:
        skew = -1 * skew
    return skew



#Function defined to Compute Color Moments for an Image
#Here image_id represents the id of the image in the dataset which is passed as 'dataset'
def computeColorMoments(image_id, dataset):
    img, label = dataset[image_id]            #Retrieving the image from the dataset by image_id

    #Resizing the image so that every image is scaled to the same size for Feature Descriptor extraction
    #The image is resized to a image of width 300 and height 100 as per the given problem description
    resized_img = img.resize((300,100))
    img_array = np.array(resized_img)         #Converting the image to numpy array to perform mathematical operations

    if (len(img_array.shape) != 3 or img_array.shape[2] != 3):           #Skipping computation of images which doesn't have all 3 colour channels
        img_array = np.stack((img_array,) * 3, axis=-1)

    grid_size = (10,10)                       #Partitioning the image with a grid of size (10,10) which is defined in the problem description
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])     #The cell size calculated based on the grid used for partition
    color_moments = []                        #Intialising an empty array to store the computed color Moments for each cell in the Image
    for i in range(10):
        for j in range(10):

            #Calculating the height and width of each cell for which the color Moments are calculated
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell = img_array[h1:h2, w1:w2]              #Slicing the intial image array to include only the cell data which can be used for color Moments computation
            moments = []
            for color_channel in range(3):
                channel_values = cell[:,:,color_channel]               #Dividing the array based on color channels R, G, B

                #Mean, Standard Deviation and Skewness Calculations using the custom defined functions for each color channel
                mean = findMean(channel_values)
                standard_deviation = findStandardDeviation(channel_values, mean)
                skewness = findSkewness(channel_values , mean)

                #Adding the color Moments of each color channel to a Temporary array
                moments.append(mean)
                moments.append(standard_deviation)
                moments.append(skewness)

            #Adding the computed color Moments of each cell to the final array
            color_moments.append(moments)

    color_moments = np.array(color_moments)
    color_moments_feature_descriptor = color_moments.reshape(10,10,3,3).flatten()       #Reshaping the computed colorMoments to (1, 900) shape for better understanding. Here 10,10 represents the cells for which color Moments are calculated and in the 3,3 matrix the row represents the colors and columns represent the mean, standard deviation and skewness values
    return color_moments_feature_descriptor


#Function defined to compute Histogram of Oriented Gradients
def computeHOG(imageId, dataset):
    img, label = dataset[imageId]
    img = img.convert("L")                            #Converting the image to gray scale as we are more focussed on the orientation than colors
    resized_image = img.resize((300,100))
    img_array = np.array(resized_image)
    grid_size = (10,10)                               #Partitioning the image with a 10,10 grid
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])
    dx_mask = np.array([[-1, 0, 1]])                  # Filter Mask we use to convolve the image array for horizantal gradient calculation
    dy_mask = dx_mask.T                               # Transposing the horizantal Filter Mask to convolve the image array for vertical gradient calculation
    grad_x = convolve2d(img_array, dx_mask, mode='same')         # X gradient calculation
    grad_y = convolve2d(img_array, dy_mask, mode='same')         # Y gradient calculation
    magnitude = np.sqrt(grad_x**2 + grad_y**2)              # Using Pythogrus formula to calculate the magnitude of the gradients
    orientation = np.arctan2(grad_y, grad_x)*180/np.pi      # Calculating the orientation of the gradients and converting it to degrees
    orientation[orientation < 0] += 360                     # Converting the negative orientation to be in the range of 0 to 360
    num_bins = 9                                            # Splitting the orientation into 9 bins each bins composing 40 degrees
    hog_features = np.zeros((10, 10, 9))
    for i in range(10):
        for j in range(10):
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell_orientation = orientation[h1:h2, w1:w2]
            cell_magnitude = magnitude[h1:h2, w1:w2]
            hist, bin_edges = np.histogram(cell_orientation, bins=num_bins, range=(0, 360), weights = cell_magnitude)       # Mapping a magintude based histogram for each cell with 9 bins based on their orientation
            hog_features[i, j, :] = hist

    hog_feature_descriptors = hog_features.flatten()                # Converting the HOG feature descriptors to 1D from 10, 10, 9 to store in JSON file
    return hog_feature_descriptors


#Function defined to retrieve the vectors from different layers of the pre-trained neural architecture : ResNet50
def computeResNet50Vectors(imageId, dataset):
    image, label = dataset[imageId]
    if (image.mode == 'L') :
      image = image.convert("RGB")
    resnet = models.resnet50(pretrained=True)          # Loading the ResNet50 model
    hook_output_avg_pool = []                          # list to store the ResNet50 avg pool layer output
    hook_output_layer3 = []                            # list to store the ResNet50 layer-3 layer output
    hook_output_fc = []                                # list to store the ResNet50 Full Connected layer output


    # Hook defined to capture the feature vectors from the avg_pool layer
    def hook_fn(module, input, output):
        hook_output_avg_pool.append(output)

    # Hook defined to capture the feature vectors from the Layer3
    def hook_fn_layer3(module, input, output):
        hook_output_layer3.append(output)

    # Hook defined to capture the feature vectors from the Fully Connected layer
    def hook_fn_fc(module, input, output):
        hook_output_fc.append(output)


    # Registering the defined hook as the forward hook on the avg pool layer on the ResNet 50 model
    avgpool_layer = resnet.avgpool
    avgpool_layer.register_forward_hook(hook_fn)

    # Registering the defined hook as the forward hook on the layer-3 on the ResNet 50 model
    layer3 = resnet.layer3
    layer3.register_forward_hook(hook_fn_layer3)

    # Registering the defined hook as the forward hook on the fully connected layer on the ResNet 50 model
    fc_layer = resnet.fc
    fc_layer.register_forward_hook(hook_fn_fc)

    # Performing tranform operations on the image to resize it and retrieve the tensors from each layer
    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])
    image = transform(image).unsqueeze(0)
    resnet(image)                # Passing the image to the ResNet Model


    #Storing the output of the hooks in the predefined lists
    avgpool_vector = hook_output_avg_pool[0][0]
    layer3_vector = hook_output_layer3[0][0]
    fc_vector = hook_output_fc[0][0]

    avgpool_vector = avgpool_vector.squeeze().detach().numpy().reshape(1024, 2)           # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_avg_pool_dimensionality_vector = np.mean(avgpool_vector, axis = 1)            # Dimensional Reduction is perfromed by averaging up the consecutive numbers and reducing the resultant array to 1D

    layer3_vector = layer3_vector.detach().numpy()                                        # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_layer3_dimensionality_vector = np.mean(layer3_vector, axis = (1, 2))          # Dimensional Reduction is perfromed by averaging up the 14, 14 slice and reducing the resultant array to 1D

    fc_vector = fc_vector.detach().numpy()                                                            # Converting the vectors from tensor to numpy to perform mathematical operations
    resnet_vector = softmax(fc_vector)
    return (reduced_avg_pool_dimensionality_vector, reduced_layer3_dimensionality_vector, fc_vector, resnet_vector)  # returning the feature descriptors of all 4 layers in a tuple



def computeFeatureDescriptors(dataset):
  data = {}
  labelsWithImages = {}

  for image_id in range(len(dataset)):
    if image_id % 2 == 0:                                               # Considering only images with even ids for training purposes
      img, label = dataset[image_id]
      print("Calculating Feature Descriptors for ", image_id)            # Just a print statement to show that program is computing descriptors for the particular image

      #Computation of Feature Descriptors
      colorMomentFD = computeColorMoments(image_id, dataset)
      HOGFeatureDescriptors = computeHOG(image_id, dataset)
      ResNet_output_vectors = computeResNet50Vectors(image_id, dataset)

      # Code to save feature descriptors in the Database
      data[image_id] = {
          'label' : label,
          'color_moments': colorMomentFD.tolist(),
          'hog':    HOGFeatureDescriptors.tolist(),
          'avg_pool': ResNet_output_vectors[0].tolist(),
          'layer_3': ResNet_output_vectors[1].tolist(),
          'fc': ResNet_output_vectors[2].tolist(),
          'resnet': ResNet_output_vectors[3].tolist()
      }

      # Code to save labels and images under each label in the dataset
      if label not in labelsWithImages:
        labelsWithImages[label] = []
      labelsWithImages[label].append(image_id)



  # Saving the updated data to the JSON file
  save_data_to_json(data, fd_json_file_path)
  save_data_to_json(labelsWithImages, label_image_mapping_json_file_path)
  print("data saved successfully")



#Loading features data from existing file or intializing data with empty dictionary
data = load_data_from_json(fd_json_file_path)
exit = False
while (not exit) :
  choice = int(input("Enter your choice 1. to compute and store feature descriptors 2. to visualize computed feature descriptors for a image_id\n"))   # User Input choice to choose any of the functions. The choice 1. computes the feature descriptors and stores them in a JSON file. The choice 2 retrieves the existing data from the JSON and displays it in a nice way
  if choice == 2:
    input_image = int(input("Enter the imageId for which you want to visualize the feature descriptors\n"))
    try :
      if (int(input_image) % 2 == 0) :     # Checking whether the image is in the dataset or not and whether it is part of our training dataset
        image_id = int(input_image)
        input_image, label = dataset[image_id]
        if (input_image is None) :          # Case where we couldn't find the input image
          print("we couldn't find an image associated with the given image_id in the caltech 101 dataset ")
          continue
        vector_space = int(input("Enter the Vector space you want to select. The choice is as follows \n 1. color_moments 2. hog 3. avg_pool 4. layer_3 5. fc 6. Resnet Softmax"))
        print("The corresponding input image is\n")
        plt.imshow(input_image)             # Displaying the given ImageId
        plt.show()

        # Case where the feature descriptors are not saved in the database
        if len(data) == 0 :
          print("The feature descriptors are not stored please choose 1 to compute the feature descriptors and store them")
          continue

        # Based on the vector space retrieving the feature descriptors and visualizing them in a human readable format
        if vector_space == 1:
          vector_space_name = 'color_moments'
          fd_descriptor = np.array(data[str(image_id)][vector_space_name]).reshape(10,10,3,3)
          print(fd_descriptor)
        elif vector_space == 2:
          vector_space_name = 'hog'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 3:
          vector_space_name = 'avg_pool'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 4:
          vector_space_name = 'layer_3'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 5:
          vector_space_name = 'fc'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 6:
          vector_space_name = 'resnet'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        else:
          print("The vector space you selected is not one of the allowed options. Please check again")
          continue
    except Exception as e:
      print(f"An error occurred: {e}")
  else:                     #If choice is anything other than 2 we go for computation of feature descriptors
    computeFeatureDescriptors(dataset)

  exit_choice = int(input("For Menu enter 1, To exit enter 2 \n"))
  if (exit_choice != 1):
        exit = True

#Imports needed to run the code
import torchvision
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import PIL
from PIL import Image
import time
from torchvision import models, transforms
from scipy.signal import convolve2d
import matplotlib.pyplot as plt
import json
from scipy.spatial.distance import minkowski
from sklearn.metrics.pairwise import cosine_similarity
from scipy.stats import pearsonr
import seaborn as sns
from scipy.special import softmax

#Function defined to find Mean
def findMean(cell_array):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + cell_array[i][j]
    N = len(cell_array)*len(cell_array[0])
    return sum/N


#Function defined to find Standard deviation using previously calculated Mean
def findStandardDeviation(cell_array, mean):
    sum = 0.0
    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),2)
    N = len(cell_array)*len(cell_array[0])
    return np.float_power((sum/N),1/2.)


#Function defined to find Skewness using previously calculated Standard Deviation
def findSkewness(cell_array , mean):
    sum = 0.0

    for i in range(len(cell_array)):
        for j in range(len(cell_array[0])):
            sum = sum + np.float_power((cell_array[i][j] - mean),3)
    N = len(cell_array)*len(cell_array[0])
    #Using a Negative indicator so that skewness can support Negative values
    neg = False
    if sum < 0:
        neg = True
        sum = sum * -1
    skew = np.float_power((sum/N),1/3.)
    if neg:
        skew = -1 * skew
    return skew



#Function defined to Compute Color Moments for an Image
#Here image_id represents the id of the image in the dataset which is passed as 'dataset'
def computeColorMoments(image_id, dataset):
    img, label = dataset[image_id]            #Retrieving the image from the dataset by image_id

    #Resizing the image so that every image is scaled to the same size for Feature Descriptor extraction
    #The image is resized to a image of width 300 and height 100 as per the given problem description
    resized_img = img.resize((300,100))
    img_array = np.array(resized_img)         #Converting the image to numpy array to perform mathematical operations

    if (len(img_array.shape) != 3 or img_array.shape[2] != 3):           #Skipping computation of images which doesn't have all 3 colour channels
        img_array = np.stack((img_array,) * 3, axis=-1)

    grid_size = (10,10)                       #Partitioning the image with a grid of size (10,10) which is defined in the problem description
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])     #The cell size calculated based on the grid used for partition
    color_moments = []                        #Intialising an empty array to store the computed color Moments for each cell in the Image
    for i in range(10):
        for j in range(10):

            #Calculating the height and width of each cell for which the color Moments are calculated
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell = img_array[h1:h2, w1:w2]              #Slicing the intial image array to include only the cell data which can be used for color Moments computation
            moments = []
            for color_channel in range(3):
                channel_values = cell[:,:,color_channel]               #Dividing the array based on color channels R, G, B

                #Mean, Standard Deviation and Skewness Calculations using the custom defined functions for each color channel
                mean = findMean(channel_values)
                standard_deviation = findStandardDeviation(channel_values, mean)
                skewness = findSkewness(channel_values , mean)

                #Adding the color Moments of each color channel to a Temporary array
                moments.append(mean)
                moments.append(standard_deviation)
                moments.append(skewness)

            #Adding the computed color Moments of each cell to the final array
            color_moments.append(moments)

    color_moments = np.array(color_moments)
    color_moments_feature_descriptor = color_moments.reshape(10,10,3,3).flatten()       #Reshaping the computed colorMoments to (1, 900) shape for better understanding. Here 10,10 represents the cells for which color Moments are calculated and in the 3,3 matrix the row represents the colors and columns represent the mean, standard deviation and skewness values
    return color_moments_feature_descriptor


#Function defined to compute Histogram of Oriented Gradients
def computeHOG(imageId, dataset):
    img, label = dataset[imageId]
    img = img.convert("L")                            #Converting the image to gray scale as we are more focussed on the orientation than colors
    resized_image = img.resize((300,100))
    img_array = np.array(resized_image)
    grid_size = (10,10)                               #Partitioning the image with a 10,10 grid
    cell_size = (img_array.shape[0]//grid_size[0], img_array.shape[1]//grid_size[1])
    dx_mask = np.array([[-1, 0, 1]])                  # Filter Mask we use to convolve the image array for horizantal gradient calculation
    dy_mask = dx_mask.T                               # Transposing the horizantal Filter Mask to convolve the image array for vertical gradient calculation
    grad_x = convolve2d(img_array, dx_mask, mode='same')         # X gradient calculation
    grad_y = convolve2d(img_array, dy_mask, mode='same')         # Y gradient calculation
    magnitude = np.sqrt(grad_x**2 + grad_y**2)              # Using Pythogrus formula to calculate the magnitude of the gradients
    orientation = np.arctan2(grad_y, grad_x)*180/np.pi      # Calculating the orientation of the gradients and converting it to degrees
    orientation[orientation < 0] += 360                     # Converting the negative orientation to be in the range of 0 to 360
    num_bins = 9                                            # Splitting the orientation into 9 bins each bins composing 40 degrees
    hog_features = np.zeros((10, 10, 9))
    for i in range(10):
        for j in range(10):
            h1,h2 = i * cell_size[0], (i + 1) * cell_size[0]
            w1,w2 = j * cell_size[1], (j + 1) * cell_size[1]
            cell_orientation = orientation[h1:h2, w1:w2]
            cell_magnitude = magnitude[h1:h2, w1:w2]
            hist, bin_edges = np.histogram(cell_orientation, bins=num_bins, range=(0, 360), weights = cell_magnitude)       # Mapping a magintude based histogram for each cell with 9 bins based on their orientation
            hog_features[i, j, :] = hist

    hog_feature_descriptors = hog_features.flatten()                # Converting the HOG feature descriptors to 1D from 10, 10, 9 to store in JSON file
    return hog_feature_descriptors


#Function defined to retrieve the vectors from different layers of the pre-trained neural architecture : ResNet50
def computeResNet50Vectors(imageId, dataset):
    image, label = dataset[imageId]
    if (image.mode == 'L') :
      image = image.convert("RGB")
    resnet = models.resnet50(pretrained=True)          # Loading the ResNet50 model
    hook_output_avg_pool = []                          # list to store the ResNet50 avg pool layer output
    hook_output_layer3 = []                            # list to store the ResNet50 layer-3 layer output
    hook_output_fc = []                                # list to store the ResNet50 Full Connected layer output


    # Hook defined to capture the feature vectors from the avg_pool layer
    def hook_fn(module, input, output):
        hook_output_avg_pool.append(output)

    # Hook defined to capture the feature vectors from the Layer3
    def hook_fn_layer3(module, input, output):
        hook_output_layer3.append(output)

    # Hook defined to capture the feature vectors from the Fully Connected layer
    def hook_fn_fc(module, input, output):
        hook_output_fc.append(output)


    # Registering the defined hook as the forward hook on the avg pool layer on the ResNet 50 model
    avgpool_layer = resnet.avgpool
    avgpool_layer.register_forward_hook(hook_fn)

    # Registering the defined hook as the forward hook on the layer-3 on the ResNet 50 model
    layer3 = resnet.layer3
    layer3.register_forward_hook(hook_fn_layer3)

    # Registering the defined hook as the forward hook on the fully connected layer on the ResNet 50 model
    fc_layer = resnet.fc
    fc_layer.register_forward_hook(hook_fn_fc)

    # Performing tranform operations on the image to resize it and retrieve the tensors from each layer
    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])
    image = transform(image).unsqueeze(0)
    resnet(image)                # Passing the image to the ResNet Model


    #Storing the output of the hooks in the predefined lists
    avgpool_vector = hook_output_avg_pool[0][0]
    layer3_vector = hook_output_layer3[0][0]
    fc_vector = hook_output_fc[0][0]


    avgpool_vector = avgpool_vector.squeeze().detach().numpy().reshape(1024, 2)           # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_avg_pool_dimensionality_vector = np.mean(avgpool_vector, axis = 1)            # Dimensional Reduction is perfromed by averaging up the consecutive numbers and reducing the resultant array to 1D

    layer3_vector = layer3_vector.detach().numpy()                                        # Converting the vectors from tensor to numpy to perform mathematical operations
    reduced_layer3_dimensionality_vector = np.mean(layer3_vector, axis = (1, 2))          # Dimensional Reduction is perfromed by averaging up the 14, 14 slice and reducing the resultant array to 1D

    fc_vector = fc_vector.detach().numpy()
    resnet_vector = softmax(fc_vector)                                                           # Converting the vectors from tensor to numpy to perform mathematical operations
    return (reduced_avg_pool_dimensionality_vector, reduced_layer3_dimensionality_vector, fc_vector, resnet_vector, resnet_vector)  # returning the feature descriptors of all 3 layers in a tuple

# JSON file paths
fd_json_file_path = '/content/drive/MyDrive/CSE515-Phase 2/Feature_Descriptors.json'     # path of the file used to store the data
label_image_mapping_json_file_path = '/content/drive/MyDrive/CSE515-Phase 2/LabelsWithImages.json'


# Function to save the data to a JSON file. If the file is not available it will create a file in the root project
def save_data_to_json(data, json_file):
    with open(json_file, 'w') as file:
        json.dump(data, file, indent=4)

# Function to load the existing data from a JSON file
def load_data_from_json(json_file):
    try:
        with open(json_file, 'r') as file:
            data = json.load(file)
    except FileNotFoundError:
        data = {}  # Initialize with an empty dictionary if the file doesn't exist
    return data


# dataset
dataset = torchvision.datasets.Caltech101(root = '/content/drive/MyDrive/CSE515-Phase 2/data', download = True) # Caltech101 Dataset will be downloaded in the mentioned path, if already downloaded it will not download again

def computeFeatureDescriptors(dataset):
  data = {}
  labelsWithImages = {}

  for image_id in range(len(dataset)):
    if image_id % 2 == 0:
      img, label = dataset[image_id]
      print("Calculating Feature Descriptors for ", image_id)            # Just a print statement to show that program is computing descriptors for the particular image

      #Computation of Feature Descriptors
      colorMomentFD = computeColorMoments(image_id, dataset)
      HOGFeatureDescriptors = computeHOG(image_id, dataset)
      ResNet_output_vectors = computeResNet50Vectors(image_id, dataset)

      # Code to save it in the Database
      data[image_id] = {
          'label' : label,
          'color_moments': colorMomentFD.tolist(),
          'hog':    HOGFeatureDescriptors.tolist(),
          'avg_pool': ResNet_output_vectors[0].tolist(),
          'layer_3': ResNet_output_vectors[1].tolist(),
          'fc': ResNet_output_vectors[2].tolist(),
          'resnet': ResNet_output_vectors[3].tolist()
      }

      if label not in labelsWithImages:
        labelsWithImages[label] = []
      labelsWithImages[label].append(image_id)



  # Saving the updated data to the JSON file
  save_data_to_json(data, fd_json_file_path)
  save_data_to_json(labelsWithImages, label_image_mapping_json_file_path)
  print("data saved successfully")

data = load_data_from_json(fd_json_file_path)
exit = False
while (not exit) :
  choice = int(input("Enter your choice 1. to compute and store feature descriptors 2. to visualize computed feature descriptors for a image_id\n"))
  if choice == 2:
    input_image = int(input("Enter the imageId for which you want to visualize the feature descriptors\n"))
    try :
      if (int(input_image) % 2 == 0) :
        image_id = int(input_image)
        input_image, label = dataset[image_id]
        if (input_image is None) :
          print("we couldn't find an image associated with the given image_id in the caltech 101 dataset ")
          continue
        vector_space = int(input("Enter the Vector space you want to select. The choice is as follows \n 1. color_moments 2. hog 3. avg_pool 4. layer_3 5. fc 6. Resnet softmax"))
        print("The corresponding input image is\n")
        plt.imshow(input_image)
        plt.show()
        if len(data) == 0 :
          print("The feature descriptors are not stored please choose 1 to compute the feature descriptors and store them")
          continue
        if vector_space == 1:
          vector_space_name = 'color_moments'
          fd_descriptor = np.array(data[str(image_id)][vector_space_name]).reshape(10,10,3,3)
          print(fd_descriptor)
        elif vector_space == 2:
          vector_space_name = 'hog'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 3:
          vector_space_name = 'avg_pool'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 4:
          vector_space_name = 'layer_3'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 5:
          vector_space_name = 'fc'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        elif vector_space == 6:
          vector_space_name = 'resnet'
          fd_descriptor = data[str(image_id)][vector_space_name]
          print(fd_descriptor)
        else:
          print("The vector space you selected is not one of the allowed options. Please check again")
          continue
    except Exception as e:
      print(f"An error occurred: {e}")
  else:
    computeFeatureDescriptors(dataset)

  time.sleep(2)
  print("\n")
  exit_choice = int(input("For Menu enter 1, To exit enter 2 \n"))
  if (exit_choice != 1):
        exit = True

data = load_data_from_json(fd_json_file_path)
for key in data.keys() :
  data[key]['resnet'] = softmax(data[key]['fc']).tolist()

save_data_to_json(data, fd_json_file_path)
print("data saved successfully")

fd_json_file_path = '/content/drive/MyDrive/CSE515-Phase 2/Feature_Descriptors.json'     # path of the file used to store the data
data = load_data_from_json(fd_json_file_path)
print(len(data))
data['0'].keys()

data['0']['resnet']

